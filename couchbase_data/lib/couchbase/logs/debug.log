[ns_server:info,2024-10-08T19:52:01.203Z,nonode@nohost:<0.155.0>:ns_server:init_logging:180]Started & configured logging
[ns_server:warn,2024-10-08T19:52:01.243Z,nonode@nohost:<0.155.0>:config_profile:load:123]Could not load profile file ("/etc/couchbase.d/config_profile") because it does not exist
[ns_server:debug,2024-10-08T19:52:01.249Z,nonode@nohost:<0.155.0>:ns_server:setup_server_profile:108]Using profile 'default': [{name,"default"},
                          {{indexer,disable_shard_affinity},true}]
[ns_server:warn,2024-10-08T19:52:01.271Z,nonode@nohost:<0.155.0>:ns_server:config_profile_continuity_checker:129]Writing config_profile '"default"' to disk.
[ns_server:info,2024-10-08T19:52:01.507Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,10},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_json_rpc,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,2},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_tls_key_log,
     [{rotation,
          [{compress,true},
           {size,10485760},
           {num_files,1},
           {buffer_size_max,13107200}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2024-10-08T19:52:01.508Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.508Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.508Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.508Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.508Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.508Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.509Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.509Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.509Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.509Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.509Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.509Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.509Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.509Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.510Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.510Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.510Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.510Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.510Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.510Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.510Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.510Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.511Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.511Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.511Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_json_rpc, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.511Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_tls_key_log, which is given from command line
[ns_server:warn,2024-10-08T19:52:01.511Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2024-10-08T19:52:01.555Z,nonode@nohost:dist_manager<0.216.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2024-10-08T19:52:01.557Z,nonode@nohost:dist_manager<0.216.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2024-10-08T19:52:01.568Z,nonode@nohost:dist_manager<0.216.0>:dist_manager:init:180]ip config not found. Looks like we're brand new node
[ns_server:info,2024-10-08T19:52:01.587Z,nonode@nohost:dist_manager<0.216.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2024-10-08T19:52:01.635Z,nonode@nohost:ssl_dist_admin_sup<0.219.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.220.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:01.636Z,nonode@nohost:ssl_dist_admin_sup<0.219.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.221.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:01.636Z,nonode@nohost:ssl_dist_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.219.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:01.642Z,nonode@nohost:tls_dist_sup<0.222.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.223.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:01.647Z,nonode@nohost:tls_dist_server_sup<0.224.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.225.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:01.647Z,nonode@nohost:tls_dist_server_sup<0.224.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.226.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:01.648Z,nonode@nohost:tls_dist_server_sup<0.224.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.227.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:01.648Z,nonode@nohost:tls_dist_sup<0.222.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.224.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:01.648Z,nonode@nohost:ssl_dist_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.222.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:01.649Z,nonode@nohost:net_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.218.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:01.652Z,nonode@nohost:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[error_logger:info,2024-10-08T19:52:01.656Z,nonode@nohost:net_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.228.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:01.663Z,nonode@nohost:net_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.229.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:01.678Z,nonode@nohost:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2024-10-08T19:52:01.678Z,nonode@nohost:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2024-10-08T19:52:01.686Z,nonode@nohost:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:01.847Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[ns_server:debug,2024-10-08T19:52:01.847Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.232.0>
[ns_server:debug,2024-10-08T19:52:01.848Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[error_logger:info,2024-10-08T19:52:01.848Z,ns_1@cb.local:net_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.230.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@cb.local',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:01.849Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.217.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@cb.local',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:01.854Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2024-10-08T19:52:01.859Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:save_node:160]saving node name '"ns_1@cb.local"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2024-10-08T19:52:01.902Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2024-10-08T19:52:01.902Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2024-10-08T19:52:01.902Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:01.907Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:01.908Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.211907>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:01.908Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.211907>,
                                  inet_tcp_dist,<0.234.0>,
                                  #Ref<0.1492486459.688390146.211910>}
[ns_server:debug,2024-10-08T19:52:01.918Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2024-10-08T19:52:01.919Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2024-10-08T19:52:01.919Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2024-10-08T19:52:01.957Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:save_address_config:151]Persisted the address successfully
[error_logger:info,2024-10-08T19:52:01.968Z,ns_1@cb.local:root_sup<0.215.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.216.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:01.980Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.237.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:01.988Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {5,15,146}
Runtime info: [{otp_release,"25"},
               {erl_version,"13.2.2.3"},
               {erl_version_long,
                   "Erlang/OTP 25 [erts-13.2.2.3] [source-15104f9619] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [jit:ns]\n"},
               {system_arch_raw,"x86_64-pc-linux-gnu"},
               {system_arch,"x86_64-pc-linux-gnu"},
               {localtime,{{2024,10,8},{19,52,1}}},
               {memory,
                   [{total,42727104},
                    {processes,10024776},
                    {processes_used,10021848},
                    {system,32702328},
                    {atom,540873},
                    {atom_used,522671},
                    {binary,138392},
                    {code,10769862},
                    {ets,2639808}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,inet_tcp,re,auth,tls_dist_server_sup,tls_dist_sup,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,cb_epmd,gen_udp,inet_hosts,dist_manager,
                    root_sup,cb_dist,path_config,config_profile,
                    ns_server_stats,calendar,ale_default_formatter,
                    'ale_logger-tls_key','ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-chronicle',
                    'ale_logger-ns_server_trace','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',erl_bits,
                    otp_internal,cb_log_counter_sink,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,esaml_util,esaml,
                    ale_error_logger_handler,timer,cpu_sup,filelib,memsup,
                    disksup,os_mon,unicode_util,string,io,release_handler,
                    alarm_handler,sasl,httpd_sup,httpc_handler_sup,
                    httpc_cookie,inets_trace,httpc_manager,httpc,
                    httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-trace_logger','ale_logger-ale_logger',
                    'ale_logger-error_logger',beam_opcodes,beam_dict,beam_asm,
                    beam_z,beam_flatten,beam_trim,beam_clean,beam_block,
                    beam_utils,beam_jump,beam_a,beam_validator,
                    beam_ssa_codegen,beam_ssa_pre_codegen,beam_ssa_throw,
                    beam_ssa_dead,beam_call_types,beam_types,beam_ssa_type,
                    beam_ssa_bc_size,beam_ssa_opt,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sys_core_fold,sys_core_inline,cerl_trees,
                    core_lib,cerl,sets,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,erpc,global_group,erl_distribution,maps,
                    rand,net_kernel,global,rpc,epp,inet_gethost_native,
                    inet_parse,inet,inet_udp,inet_config,inet_db,unicode,os,
                    gb_trees,gb_sets,binary,beam_lib,peer,erl_anno,
                    erl_features,proplists,erl_scan,queue,logger_olp,
                    logger_proxy,error_handler,error_logger,application,
                    application_master,code_server,ets,application_controller,
                    code,gen_event,file,kernel,filename,logger_filters,logger,
                    logger_server,logger_backend,erl_parse,logger_config,
                    file_io_server,erl_lint,file_server,supervisor,
                    logger_simple_h,erl_eval,heart,proc_lib,gen,gen_server,
                    lists,persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{stdlib,"ERTS  CXC 138 10","4.3.1.2"},
                    {ns_server,"Couchbase server","7.6.3-4200-enterprise"},
                    {inets,"INETS  CXC 138 49","8.3.1.2"},
                    {public_key,"Public key infrastructure","1.13.3.1"},
                    {crypto,"CRYPTO","5.1.4.1"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.21","5.0.21"},
                    {ssl,"Erlang/OTP SSL application","10.9.1.2"},
                    {sasl,"SASL  CXC 138 11","4.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {esaml,"SAML Server Provider library for erlang","4.4.0"},
                    {os_mon,"CPO  CXC 138 46","2.8.2"},
                    {xmerl,"XML parser","1.3.31.1"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {kernel,"ERTS  CXC 138 10","8.5.4.2"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,159},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,standard_error,net_kernel,kernel_refc,
                    dtls_listener_sup,ale_stats_events,'sink-cb_log_counter',
                    esaml_ets_table_owner,ssl_dist_admin_sup,tls_server_sup,
                    ssl_connection_sup,erts_code_purger,
                    ssl_upgrade_server_session_cache_sup,erl_prim_loader,
                    global_group,httpd_sup,'sink-disk_debug',
                    ssl_upgrade_server_session_cache_sup_dist,init,
                    application_controller,ale_sup,tls_client_ticket_store,
                    erl_signal_server,'sink-disk_trace',
                    dtls_server_session_cache_sup,logger_sup,tls_sup,
                    dist_manager,ssl_pem_cache_dist,inets_sup,
                    'sink-disk_tls_key_log',cb_dist,cpu_sup,ale,
                    ns_server_cluster_sup,httpc_manager,rex,ssl_dist_sup,
                    ssl_sup,esaml,tls_dist_connection_sup,'sink-disk_error',
                    tls_server_session_ticket_sup_dist,ssl_listen_tracker_sup,
                    os_mon_sup,'sink-disk_reports',dtls_sup,logger_proxy,
                    tls_dist_sup,tls_server_session_ticket_sup,
                    'sink-disk_metakv',auth,disksup,ssl_manager,'sink-ns_log',
                    kernel_sup,inet_db,user,'sink-disk_default',
                    ssl_listen_tracker_sup_dist,ale_dynamic_sup,
                    logger_std_h_ssl_handler,'sink-disk_stats',logger,
                    standard_error_sup,httpc_profile_sup,
                    logger_handler_watcher,sasl_safe_sup,ssl_admin_sup,
                    kernel_safe_sup,release_handler,'sink-disk_access_int',
                    ssl_server_session_cache_sup,global_name_server,
                    file_server_2,root_sup,lhttpc_manager,alarm_handler,
                    httpc_sup,dtls_server_sup,global_group_check,
                    tls_dist_server_sup,'sink-disk_json_rpc',local_tasks,
                    ssl_pem_cache,memsup,dtls_connection_sup,'sink-disk_xdcr',
                    socket_registry,code_server,httpc_handler_sup,sasl_sup,
                    net_sup,ssl_manager_dist,tls_connection_sup,
                    'sink-disk_access']},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,22}]
[ns_server:info,2024-10-08T19:52:02.008Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:log_os_info:start_link:21]Manifest:
["<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\" />",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"9ead6b88adbf8d6131e5ae7a3a699c477a3b4195\" groups=\"kv\" />",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"726ac6511a3ea5e37820d4f8dd9fbc320ca958b7\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"fd2c99e97664c8c098eabd603e209027351abdb8\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"192d7500ba2a7b5281d2c61af126c8027bbb858d\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"cedf9d4ec929eac7e61f8e86488aeac5402c8563\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"trinity\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4200\" />",
 "    <annotation name=\"VERSION\" value=\"7.6.3\" />","  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"88bca7f661e11bf773ca181f7863e4966f318ac9\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"bf73856d460ac1aeb60ea84dcc9e8cf68116eead\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-ui\" revision=\"704db180d01de15f70cacc9fc11c5d8d8d4ff965\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"a9992170165a1d330cb5a9918a29d5bd417c5e46\" groups=\"backup\" />",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"f1d0272decc7f1b445b08e56e1f75e99f743aa90\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"cbft\" revision=\"69d32cca4a8eca6e5aad5dad689795ab72ecdd6e\" />",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"258e3829db59f06a202ea2435c776a351a590eba\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"cbgt\" revision=\"b7dd01a11c5c56fbca88b9b950d8eca4dacce36f\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"fb656c91554a97318c44f58e3cc7f166f1eef4fc\" />",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"8d1feeb0d8b15e2b6a4c1a417addfd159b422a71\" />",
 "  <project name=\"client_golang\" path=\"godeps/src/github.com/prometheus/client_golang\" remote=\"couchbasedeps\" revision=\"2e1c4818ccfdcf953ce399cadad615ff2bed968c\" upstream=\"refs/tags/v1.12.1\" dest-branch=\"refs/tags/v1.12.1\" />",
 "  <project name=\"client_model\" path=\"godeps/src/github.com/prometheus/client_model\" remote=\"couchbasedeps\" revision=\"6dc836ede0b5b08c61893c3ffeb474498b18bb83\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\" />",
 "  <project name=\"common\" path=\"godeps/src/github.com/prometheus/common\" remote=\"couchbasedeps\" revision=\"902cb39e6c079571d32c2db8da220da13c11b562\" upstream=\"refs/tags/v0.33.0\" dest-branch=\"refs/tags/v0.33.0\" />",
 "  <project name=\"couchbase-cli\" revision=\"941f6d7bbac8f8a42870c3f5459376b9f19ef1fd\" groups=\"kv\" />",
 "  <project name=\"couchdb\" revision=\"3e5b8f248d77dd9317b36b50eed2567bcfb5f4cf\" dest-branch=\"unstable\" />",
 "  <project name=\"couchdbx-app\" revision=\"702647dd015e7443de9cdb789806351774e85463\" groups=\"notdefault,packaging\" />",
 "  <project name=\"couchstore\" revision=\"ce7305bab3feb64bd2504f34d24a1419008e8bda\" groups=\"kv\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"eb61739cd99fb244c7cd188d3c5bae54824e781d\" upstream=\"refs/tags/v0.15.0\" dest-branch=\"refs/tags/v0.15.0\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"cf3254d7dfb042192c9a23bd2e64a281c32a29d8\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"047b756132464b8f756cc35e02a15b5f498f80d5\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"5425f180a0756868524081f889ab224cfc10b70d\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project name=\"forestdb\" revision=\"9efe6d75d7d61e742af70fb47fe97ad1d04ba86f\" groups=\"backup\" />",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\" />",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"cf1acfcdf4751e0554ffa765d03e479ec491cad6\" />",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"d6e17ad2b9a218e82569e09b761c226fa8df726a\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\" />",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"2d3ecc3de903a5e4d0bc9181adedb5e637f83435\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"8db06ae62940835d35db4de075bd68f0e00ea6b7\" groups=\"bsl\" />",
 "  <project name=\"golang\" remote=\"couchbaselabs\" revision=\"4dd1b189981c94835b61c1607ca765e88604ce5a\" groups=\"kv\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"golang_protobuf_extensions\" path=\"godeps/src/github.com/matttproud/golang_protobuf_extensions\" remote=\"couchbasedeps\" revision=\"c182affec369e30f25d3eb8cd8a478dee585ae7d\" />",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"689b8f03386ba2e7bac304bfd3a525b1e1427675\" groups=\"backup\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"816f7d6346c9fc5473c4a11e3efe9ed29a2f7f72\" />",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"30adfca73d8113b5b217097414d7c3adeeef849a\" groups=\"bsl\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"4c570a31e5a6f3e087e147edf781022352497f64\" groups=\"bsl\" />",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\" />",
 "  <project name=\"hebrew\" remote=\"couchbase-priv\" revision=\"c57616b187889a5318688f49817ccaceb9c098b9\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"8b684f12ad8e11cbae240b4a45da818a6d3e2b53\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"bsl\" />",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\" />",
 "  <project name=\"kv_engine\" revision=\"42f4bd12173f83a37b1f594e6a5f01f27b042c18\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"kv,bsl\" />",
 "  <project name=\"libcouchbase\" revision=\"684931e59cd87e0c6292e8142c2b18897be5b10c\" />",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"86c2233ab8780e7aa71e0199bb957dcda2cf6cd1\" groups=\"notdefault,enterprise,kv_ee\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"a1fc533c18e5094ce75262c9e711d7189d256cd2\" groups=\"bsl\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"b70d849f0207f7cfe7ebf32b2db35b534929e041\" groups=\"bsl\" />",
 "  <project name=\"ns_server\" revision=\"d2f0bd13f5a55877c17b9e38ae0e2aa61d24d813\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"bsl\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\" />",
 "  <project name=\"perks\" path=\"godeps/src/github.com/beorn7/perks\" remote=\"couchbasedeps\" revision=\"37c8de3658fcb183f997c4e13e8337516ab753e6\" upstream=\"refs/tags/v1.0.1\" dest-branch=\"refs/tags/v1.0.1\" />",
 "  <project name=\"phosphor\" revision=\"c0a034fe407eec4723f2e01db2d72762efdbc276\" groups=\"bsl,kv\" />",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\" />",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"29364d1747a4ef55cba034460ac173a97006b5f5\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"bsl,notdefault,enterprise\" />",
 "  <project name=\"platform\" revision=\"a158d359293665b6251973868fdc42c3b642474c\" groups=\"bsl,kv\" />",
 "  <project name=\"procfs\" path=\"godeps/src/github.com/prometheus/procfs\" remote=\"couchbasedeps\" revision=\"76fc8b844e3a18c31bf689e4fe7efdd5a2f41298\" />",
 "  <project name=\"product-metadata\" revision=\"1bd027c34f33919f7005ddae0ba032a3120fe776\" groups=\"notdefault,packaging\" />",
 "  <project name=\"product-texts\" revision=\"ec39f811376df6d18e56c81873fd565093666505\" upstream=\"master\" dest-branch=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"d04d7b157bb510b1e0c10132224b616ac0e26b17\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\" />",
 "  <project name=\"protobuf-go\" path=\"godeps/src/google.golang.org/protobuf\" remote=\"couchbasedeps\" revision=\"32051b4f86e54c2142c7c05362c6e96ae3454a1c\" upstream=\"refs/tags/v1.28.0\" dest-branch=\"refs/tags/v1.28.0\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"ca77a66aa1bef48951ca9fd8e8d6a67e0fb3703e\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"bsl\" />",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"6924a352019351c746fe08a2cf9a1993b54093e8\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"query-ui\" revision=\"abcc90e091c46ad74a59bb2fe768b6f09864ddbf\" groups=\"bsl\" />",
 "  <project name=\"regulator\" path=\"goproj/src/github.com/couchbase/regulator\" remote=\"couchbase-priv\" revision=\"4ef404748ecc34fd87bdebc56074ebe99d240464\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"sigar\" revision=\"2da0c123cfb45ae39e76e730bd960db8812e3f20\" groups=\"kv\" />",
 "  <project name=\"simdutf\" path=\"third_party/simdutf\" remote=\"couchbasedeps\" revision=\"4a212616ba23c65c7048f9604faccbff5353300f\" upstream=\"refs/tags/v3.2.14\" dest-branch=\"refs/tags/v3.2.14\" groups=\"kv\" />",
 "  <project name=\"subjson\" revision=\"a619faccb30e43a4bc0708ee11b1b24abb349f18\" groups=\"bsl,kv\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"d36c6a25d886e7c9975d5bf247ac24887ba6da37\" />",
 "  <project name=\"testrunner\" revision=\"d84494d49eaf13e1dbd96e0c2e93d64a0df9adad\" upstream=\"trinity\" dest-branch=\"trinity\" />",
 "  <project name=\"tlm\" revision=\"0c610d8e4738567440ffb1f557dfa15bff81b99d\" upstream=\"7.6.3\" dest-branch=\"7.6.3\" groups=\"bsl,kv\">",
 "    <copyfile src=\"Build.sh\" dest=\"Build.sh\" />",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\" />",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\" />",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\" />",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\" />",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\" />",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vbmap\" revision=\"6cce93c4af4497d8108c3ed31b84d7139321cc82\" />",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"19881dacfffb6d834a7aaa4a6d1925a904ea387f\" groups=\"notdefault,packaging\" />",
 "  <project name=\"xxhash\" path=\"goproj/src/github.com/cespare/xxhash\" remote=\"couchbasedeps\" revision=\"e7a6b52374f7e2abfb8abb27249d53a1997b09a7\" upstream=\"refs/tags/v2.1.2\" dest-branch=\"refs/tags/v2.1.2\" />",
 "</manifest>"]

[error_logger:info,2024-10-08T19:52:02.017Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.238.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.021Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.239.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:02.034Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2024-10-08T19:52:02.086Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.246.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.086Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.247.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.090Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.248.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.094Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.249.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.113Z,ns_1@cb.local:chronicle_agent_sup<0.250.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.251.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.113Z,ns_1@cb.local:chronicle_agent_sup<0.250.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.252.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2024-10-08T19:52:02.336Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_storage:open_logs:180]Reading log files [{0,
                    "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log"}]
[chronicle:info,2024-10-08T19:52:02.349Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_storage:open_current_log:232]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2024-10-08T19:52:02.349Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_storage:create_log:280]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2024-10-08T19:52:02.373Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[error_logger:info,2024-10-08T19:52:02.406Z,ns_1@cb.local:chronicle_agent_sup<0.250.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.253.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.406Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.250.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:02.414Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.255.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:02.415Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@cb.local'

[ns_server:debug,2024-10-08T19:52:02.419Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_local:init:58]Chronicle state is: not_provisioned
[ns_server:debug,2024-10-08T19:52:02.419Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_local:provision:139]Provision chronicle on this node
[chronicle:debug,2024-10-08T19:52:02.425Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_agent:handle_provision:1199]Provisioning with history <<"05f558c2457ead34373f1c979f713946">>. Config:
{config,undefined,0,undefined,
        #{'ns_1@cb.local' =>
              #{id => <<"8e86466e21d6eeba2a219ad9e7790e30">>,role => voter}},
        undefined,
        #{chronicle_config_rsm => {rsm_config,chronicle_config_rsm,[]},
          kv => {rsm_config,chronicle_kv,[]}},
        #{},undefined,
        [{<<"05f558c2457ead34373f1c979f713946">>,0}]}
[error_logger:info,2024-10-08T19:52:02.436Z,ns_1@cb.local:<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.256.0>,dynamic_supervisor}
    started: [{pid,<0.257.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.446Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.259.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.450Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.260.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.450Z,ns_1@cb.local:<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.256.0>,dynamic_supervisor}
    started: [{pid,<0.258.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:02.455Z,ns_1@cb.local:<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.256.0>,dynamic_supervisor}
    started: [{pid,<0.261.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2024-10-08T19:52:02.483Z,ns_1@cb.local:chronicle_config_rsm<0.265.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2024-10-08T19:52:02.507Z,ns_1@cb.local:chronicle_server<0.261.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.265.0>
[error_logger:info,2024-10-08T19:52:02.507Z,ns_1@cb.local:<0.264.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.264.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.265.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"8e86466e21d6eeba2a219ad9e7790e30">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.507Z,ns_1@cb.local:<0.263.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.263.0>,dynamic_supervisor}
    started: [{pid,<0.264.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"8e86466e21d6eeba2a219ad9e7790e30">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:02.514Z,ns_1@cb.local:<0.267.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.267.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.268.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2024-10-08T19:52:02.522Z,ns_1@cb.local:kv<0.269.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[chronicle:debug,2024-10-08T19:52:02.545Z,ns_1@cb.local:chronicle_server<0.261.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.269.0>
[error_logger:info,2024-10-08T19:52:02.546Z,ns_1@cb.local:<0.267.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.267.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.269.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"8e86466e21d6eeba2a219ad9e7790e30">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.546Z,ns_1@cb.local:<0.263.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.263.0>,dynamic_supervisor}
    started: [{pid,<0.267.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"8e86466e21d6eeba2a219ad9e7790e30">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:02.547Z,ns_1@cb.local:<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.256.0>,dynamic_supervisor}
    started: [{pid,<0.262.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2024-10-08T19:52:02.550Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_upgrade:initialize:70]Setup initial chronicle content [{set,counters,[]},
                                 {set,auto_reprovision_cfg,
                                  [{enabled,true},{max_nodes,1},{count,0}]},
                                 {set,bucket_names,[]},
                                 {set,nodes_wanted,['ns_1@cb.local']},
                                 {set,server_groups,
                                  [[{uuid,<<"0">>},
                                    {name,<<"Group 1">>},
                                    {nodes,['ns_1@cb.local']}]]},
                                 {set,
                                  {node,'ns_1@cb.local',membership},
                                  active},
                                 {set,autocompaction,
                                  [{database_fragmentation_threshold,
                                    {30,undefined}},
                                   {view_fragmentation_threshold,
                                    {30,undefined}},
                                   {magma_fragmentation_percentage,50}]}]
[chronicle:debug,2024-10-08T19:52:02.733Z,ns_1@cb.local:chronicle_leader<0.257.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2024-10-08T19:52:02.733Z,ns_1@cb.local:<0.271.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"05f558c2457ead34373f1c979f713946">>
Log position: {{1,'ns_1@cb.local'},1}
Peers: ['ns_1@cb.local']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@cb.local'],
                                  [],[],[]}}}}
[chronicle:info,2024-10-08T19:52:02.734Z,ns_1@cb.local:<0.271.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2024-10-08T19:52:02.734Z,ns_1@cb.local:chronicle_leader<0.257.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {2,'ns_1@cb.local'} (history id <<"05f558c2457ead34373f1c979f713946">>)
[chronicle:debug,2024-10-08T19:52:02.758Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_agent:handle_establish_term:1529]Accepted term {2,'ns_1@cb.local'} in history <<"05f558c2457ead34373f1c979f713946">>
[chronicle:debug,2024-10-08T19:52:02.758Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:establish_term_init:367]Going to establish term {2,'ns_1@cb.local'} (history id <<"05f558c2457ead34373f1c979f713946">>).
Quorum peers: ['ns_1@cb.local']
Metadata:
{metadata,'ns_1@cb.local',<<"8e86466e21d6eeba2a219ad9e7790e30">>,
          <<"05f558c2457ead34373f1c979f713946">>,
          {1,'ns_1@cb.local'},
          {1,'ns_1@cb.local'},
          1,1,
          {log_entry,<<"05f558c2457ead34373f1c979f713946">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"8e86466e21d6eeba2a219ad9e7790e30">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"05f558c2457ead34373f1c979f713946">>,0}]}},
          {log_entry,<<"05f558c2457ead34373f1c979f713946">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"8e86466e21d6eeba2a219ad9e7790e30">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"05f558c2457ead34373f1c979f713946">>,0}]}},
          undefined}
[chronicle:debug,2024-10-08T19:52:02.759Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {2,'ns_1@cb.local'} (history id <<"05f558c2457ead34373f1c979f713946">>) successfully.
Votes: ['ns_1@cb.local']
[chronicle:debug,2024-10-08T19:52:02.759Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {2,'ns_1@cb.local'} in history <<"05f558c2457ead34373f1c979f713946">>
[chronicle:debug,2024-10-08T19:52:02.768Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {2,'ns_1@cb.local'} in history <<"05f558c2457ead34373f1c979f713946">> is ready. Committed seqno: 2
[chronicle:info,2024-10-08T19:52:02.768Z,ns_1@cb.local:chronicle_leader<0.257.0>:chronicle_leader:handle_note_term_status:596]Term {2,'ns_1@cb.local'} established.
[ns_server:info,2024-10-08T19:52:02.798Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_upgrade:initialize:72]Chronicle content was initialized. Rev = {<<"05f558c2457ead34373f1c979f713946">>,
                                          3}.
[error_logger:info,2024-10-08T19:52:02.799Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.240.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:02.803Z,ns_1@cb.local:ns_cluster<0.274.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[error_logger:info,2024-10-08T19:52:02.804Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.274.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:02.808Z,ns_1@cb.local:sigar<0.276.0>:sigar:spawn_sigar:134]Spawning sigar process 'portsigar for ns_1@cb.local'("/opt/couchbase/bin/sigar_port") with babysitter pid: 42 and log file "/opt/couchbase/var/lib/couchbase/logs/sigar_port.log"
[error_logger:info,2024-10-08T19:52:02.809Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.276.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:02.812Z,ns_1@cb.local:ns_config_sup<0.277.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2024-10-08T19:52:02.815Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.278.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.815Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.279.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:02.815Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.280.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:02.983Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1113]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2024-10-08T19:52:02.985Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1127]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2024-10-08T19:52:02.986Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1132]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2024-10-08T19:52:02.998Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1135]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2024-10-08T19:52:03.010Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1157]Here's full dynamic config we loaded + static & default config:
[{resource_management,
  [{bucket,
    [{resident_ratio,
      [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
     {data_size,[{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
   {index,[]},
   {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
   {disk_usage,[{enabled,false},{maximum,96}]},
   {collections_per_quota,[{enabled,false},{maximum,1}]}]},
 {{node,'ns_1@cb.local',index_dir},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@cb.local',database_dir},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   false]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   8092]},
 {{node,'ns_1@cb.local',backup_grpc_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9124]},
 {{node,'ns_1@cb.local',backup_https_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   18097]},
 {{node,'ns_1@cb.local',backup_http_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   8097]},
 {{node,'ns_1@cb.local',prometheus_http_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9123]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9999]},
 {{node,'ns_1@cb.local',memcached_prometheus},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   11280]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {health_monitor_refresh_interval,[]},
 {service_orchestrator_weight,
  [{kv,10000},
   {index,1000},
   {fts,1000},
   {cbas,1000},
   {n1ql,100},
   {eventing,100},
   {backup,10}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]}]},
 {{node,'ns_1@cb.local',event_log},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1217},
 {fts_memory_quota,417},
 {memory_quota,1419},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,true},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {tcp_user_timeout,tcp_user_timeout},
     {always_collect_trace_info,always_collect_trace_info},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {prune_age,0},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<219,227,234,60,174,185,173,63,75,103,129,138>>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,[]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   {7,6}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636322}}]}|
   <<"ad4331accf06ea1cb0fdd46980abac3e">>]}]
[ns_server:debug,2024-10-08T19:52:03.028Z,ns_1@cb.local:tombstone_keeper<0.278.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2024-10-08T19:52:03.031Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.281.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:03.034Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.284.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:03.034Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.285.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:03.035Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.277.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:03.040Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',n2n_client_cert_auth} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636323}}]}|false]
[ns_server:debug,2024-10-08T19:52:03.040Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636323}}]},
 {inet,false}]
[error_logger:info,2024-10-08T19:52:03.040Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.287.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:03.040Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636323}}]}|false]
[ns_server:debug,2024-10-08T19:52:03.041Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636323}}]}|inet]
[error_logger:info,2024-10-08T19:52:03.050Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.290.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:03.071Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.293.0>},
              {id,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:03.084Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.298.0>},
              {id,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:03.087Z,ns_1@cb.local:menelaus_barrier<0.299.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2024-10-08T19:52:03.087Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.299.0>},
              {id,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:03.094Z,ns_1@cb.local:<0.302.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    rest_lhttpc_pool} with delay of 1000
[error_logger:info,2024-10-08T19:52:03.097Z,ns_1@cb.local:<0.303.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.303.0>,suppress_max_restart_intensity}
    started: [{pid,<0.304.0>},
              {id,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:03.098Z,ns_1@cb.local:<0.301.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.301.0>,suppress_max_restart_intensity}
    started: [{pid,<0.302.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      rest_lhttpc_pool}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,rest_lhttpc_pool},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => rest_lhttpc_pool,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [lhttpc_manager],
                          restart => permanent,shutdown => 1000,
                          start =>
                              {lhttpc_manager,start_link,
                                  [[{name,rest_lhttpc_pool},
                                    {connection_timeout,120000},
                                    {pool_size,20}]]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:03.098Z,ns_1@cb.local:rest_lhttpc_pool_sup<0.300.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.301.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,rest_lhttpc_pool}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => rest_lhttpc_pool,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [lhttpc_manager],
                         restart => permanent,shutdown => 1000,
                         start =>
                             {lhttpc_manager,start_link,
                                 [[{name,rest_lhttpc_pool},
                                   {connection_timeout,120000},
                                   {pool_size,20}]]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:03.099Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.300.0>},
              {id,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:03.107Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.305.0>},
              {id,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:03.118Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.306.0>},
              {id,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:03.121Z,ns_1@cb.local:ns_ssl_services_sup<0.307.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.308.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:03.145Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2024-10-08T19:52:03.419Z,ns_1@cb.local:<0.313.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2024-10-08T19:52:03.419Z,ns_1@cb.local:<0.313.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2024-10-08T19:52:03.420Z,ns_1@cb.local:<0.313.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2024-10-08T19:52:03.451Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_server_cert:generate_cert_and_pkey:155]Generated certificate and private key in 295050 us
[ns_server:debug,2024-10-08T19:52:03.600Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:847]Updating CA file with 1 certificates
[ns_server:info,2024-10-08T19:52:03.684Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:850]CA file updated: 1 cert(s) written
[ns_server:info,2024-10-08T19:52:03.686Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate node_cert because there are no certs on this node
[ns_server:debug,2024-10-08T19:52:03.942Z,ns_1@cb.local:<0.318.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2024-10-08T19:52:03.943Z,ns_1@cb.local:<0.318.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2024-10-08T19:52:03.943Z,ns_1@cb.local:<0.318.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2024-10-08T19:52:04.033Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:save_certs:968]New node_cert and pkey are written to tmp file
[ns_server:info,2024-10-08T19:52:04.152Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:save_certs_phase2:995]node_cert cert and pkey files updated
[ns_server:debug,2024-10-08T19:52:04.153Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636324}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,63966829923},
 {verified_with,<<107,97,130,133,53,205,223,106,145,182,94,117,57,80,237,76>>},
 {load_timestamp,63895636323},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIF/ySN1hftfUwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5MDY1ZDg2MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTA2NWQ4\nNjAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCi4Io+Lx0/MvtSWM9r\ng9weQQyPuR+scTsWrjbvOd2h+OodXtpOwYrck8eoUTp8GB2fWCkA8pVJFvWycrST\nFO9thXEnLAH"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIF/ySN2nk2IIwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5MDY1ZDg2MDAeFw0yNDEwMDcxOTUyMDNaFw0y\nNzAxMTAxOTUyMDNaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAM7jH/fc\ngnTfX2bo2oBApAyRR03NfveCn/YquroVujAXhBmKhviR7+yQmKZivcz3XWxav2cs\nATYHehU"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"127.0.0.1"}]
[ns_server:info,2024-10-08T19:52:04.205Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:should_regenerate_certs:899]Should regenerate client_cert because there are no certs on this node
[ns_server:debug,2024-10-08T19:52:04.497Z,ns_1@cb.local:<0.324.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2024-10-08T19:52:04.497Z,ns_1@cb.local:<0.324.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2024-10-08T19:52:04.497Z,ns_1@cb.local:<0.324.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2024-10-08T19:52:04.548Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:save_certs:968]New client_cert and pkey are written to tmp file
[ns_server:info,2024-10-08T19:52:04.667Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:save_certs_phase2:995]client_cert cert and pkey files updated
[ns_server:debug,2024-10-08T19:52:04.667Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',client_cert} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636324}}]},
 {subject,<<"CN=Couchbase Internal Client (2658606)">>},
 {not_after,63966829924},
 {verified_with,<<107,97,130,133,53,205,223,106,145,182,94,117,57,80,237,76>>},
 {load_timestamp,63895636324},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIF/ySN1hftfUwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5MDY1ZDg2MDAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgOTA2NWQ4\nNjAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCi4Io+Lx0/MvtSWM9r\ng9weQQyPuR+scTsWrjbvOd2h+OodXtpOwYrck8eoUTp8GB2fWCkA8pVJFvWycrST\nFO9thXEnLAH"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWDCCAkCgAwIBAgIIF/ySN4oECmMwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA5MDY1ZDg2MDAeFw0yNDEwMDcxOTUyMDRaFw0y\nNzAxMTAxOTUyMDRaMC4xLDAqBgNVBAMTI0NvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDI2NTg2MDYpMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyoMB\n2GcJEmjSXbsTu+MhzkSrbsUUqBba6uL+oYP3ccuZ68gIye9OY1BuLIuJY9YQxUbs\nVMECTiS"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2024-10-08T19:52:04.778Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588800000
[ns_server:info,2024-10-08T19:52:04.779Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:handle_info:764]cert_and_pkey changed
[error_logger:info,2024-10-08T19:52:04.779Z,ns_1@cb.local:ns_ssl_services_sup<0.307.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.309.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:04.825Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,cb_dist_tls,
                                     client_cert_event,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2024-10-08T19:52:04.825Z,ns_1@cb.local:<0.343.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service client_cert_event
[ns_server:debug,2024-10-08T19:52:04.825Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2024-10-08T19:52:04.825Z,ns_1@cb.local:<0.346.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service server_cert_event
[ns_server:debug,2024-10-08T19:52:04.832Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2024-10-08T19:52:04.833Z,ns_1@cb.local:<0.344.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service cb_dist_tls
[error_logger:info,2024-10-08T19:52:04.842Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998017,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:04.843Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:04.843Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212235>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:04.843Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212235>,
                                  inet_tcp_dist,<0.347.0>,
                                  #Ref<0.1492486459.688390148.211042>}
[ns_server:debug,2024-10-08T19:52:04.845Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212235>,
                               inet_tcp_dist,<0.347.0>,
                               #Ref<0.1492486459.688390148.211042>}
[error_logger:info,2024-10-08T19:52:04.845Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.347.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:04.845Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:04.845Z,ns_1@cb.local:<0.341.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2024-10-08T19:52:04.845Z,ns_1@cb.local:<0.341.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:warn,2024-10-08T19:52:04.848Z,ns_1@cb.local:<0.345.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2024-10-08T19:52:04.863Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2024-10-08T19:52:04.863Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2024-10-08T19:52:04.864Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2024-10-08T19:52:04.864Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2024-10-08T19:52:04.865Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[error_logger:info,2024-10-08T19:52:04.885Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.350.0>},
              {id,timer_server},
              {mfargs,{timer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:04.934Z,ns_1@cb.local:<0.331.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,23,252,146,55,88,95,
              181,245,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,57,48,54,53,100,56,54,48,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,57,48,54,
              53,100,56,54,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,162,224,138,62,47,
              29,63,50,251,82,88,207,107,131,220,30,65,12,143,185,31,172,113,
              59,22,174,54,239,57,221,161,248,234,29,94,218,78,193,138,220,
              147,199,168,81,58,124,24,29,159,88,41,0,242,149,73,22,245,178,
              114,180,147,20,239,109,133,113,39,44,1,245,32,115,12,103,191,
              191,243,103,66,41,139,224,181,104,75,204,233,190,254,35,202,
              119,119,161,149,233,229,112,254,211,9,64,4,128,202,232,24,168,
              24,255,52,201,191,28,153,207,56,236,83,15,15,179,38,49,16,223,
              57,145,180,191,222,142,136,118,138,41,179,80,34,13,67,166,242,
              138,186,102,149,120,148,184,38,173,189,15,12,141,149,33,172,
              110,178,14,57,167,22,20,187,34,114,177,168,196,187,61,28,151,
              63,173,196,85,194,96,201,47,89,193,132,208,82,40,193,101,124,
              93,134,155,178,23,194,48,21,65,100,197,113,23,53,81,192,121,
              199,175,193,48,89,188,238,196,219,32,96,177,93,13,22,26,184,
              143,95,109,187,229,244,157,164,131,7,48,179,19,116,224,19,216,
              129,216,95,93,133,41,207,84,108,171,188,59,2,3,1,0,1,163,66,48,
              64,48,14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,
              1,1,255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,72,161,1,
              41,120,181,194,241,208,64,236,198,92,76,179,173,233,21,231,254,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,52,52,
              146,106,72,62,32,98,181,69,110,40,169,232,133,53,197,31,60,181,
              173,120,160,236,167,241,41,65,46,193,79,89,55,216,185,123,12,
              180,12,71,15,13,196,130,207,43,31,80,250,38,78,161,158,225,135,
              94,193,223,148,163,54,119,251,117,87,116,0,14,201,151,110,5,
              233,140,1,86,114,128,105,124,58,23,109,150,44,41,180,75,181,
              225,108,130,239,76,9,46,148,135,91,156,180,49,72,95,110,43,32,
              159,147,196,90,147,0,227,199,37,225,63,78,85,229,89,193,26,97,
              16,22,25,39,153,0,84,250,167,152,251,171,19,208,62,232,112,93,
              36,180,232,230,213,223,174,110,206,150,243,101,52,71,157,158,
              142,252,220,45,231,10,117,167,15,196,20,182,13,246,134,252,64,
              178,5,61,144,221,68,188,229,32,164,13,241,186,52,137,12,145,
              121,181,74,136,71,18,134,162,10,155,127,128,235,175,122,153,
              232,20,24,239,21,233,186,200,17,188,26,143,207,56,182,94,16,
              216,89,169,20,150,30,62,242,178,115,191,219,95,124,6,170,55,
              197,150,139,192,234,170,16,80,196,99,216,254,17>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2024-10-08T19:52:04.937Z,ns_1@cb.local:<0.331.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.331.0>,menelaus_web}
    started: [{pid,<0.351.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:04.942Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2024-10-08T19:52:04.942Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2024-10-08T19:52:04.943Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2024-10-08T19:52:04.943Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2024-10-08T19:52:04.944Z,ns_1@cb.local:<0.331.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2024-10-08T19:52:04.949Z,ns_1@cb.local:<0.331.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,23,252,146,55,88,95,
              181,245,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,57,48,54,53,100,56,54,48,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,57,48,54,
              53,100,56,54,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,162,224,138,62,47,
              29,63,50,251,82,88,207,107,131,220,30,65,12,143,185,31,172,113,
              59,22,174,54,239,57,221,161,248,234,29,94,218,78,193,138,220,
              147,199,168,81,58,124,24,29,159,88,41,0,242,149,73,22,245,178,
              114,180,147,20,239,109,133,113,39,44,1,245,32,115,12,103,191,
              191,243,103,66,41,139,224,181,104,75,204,233,190,254,35,202,
              119,119,161,149,233,229,112,254,211,9,64,4,128,202,232,24,168,
              24,255,52,201,191,28,153,207,56,236,83,15,15,179,38,49,16,223,
              57,145,180,191,222,142,136,118,138,41,179,80,34,13,67,166,242,
              138,186,102,149,120,148,184,38,173,189,15,12,141,149,33,172,
              110,178,14,57,167,22,20,187,34,114,177,168,196,187,61,28,151,
              63,173,196,85,194,96,201,47,89,193,132,208,82,40,193,101,124,
              93,134,155,178,23,194,48,21,65,100,197,113,23,53,81,192,121,
              199,175,193,48,89,188,238,196,219,32,96,177,93,13,22,26,184,
              143,95,109,187,229,244,157,164,131,7,48,179,19,116,224,19,216,
              129,216,95,93,133,41,207,84,108,171,188,59,2,3,1,0,1,163,66,48,
              64,48,14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,
              1,1,255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,72,161,1,
              41,120,181,194,241,208,64,236,198,92,76,179,173,233,21,231,254,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,52,52,
              146,106,72,62,32,98,181,69,110,40,169,232,133,53,197,31,60,181,
              173,120,160,236,167,241,41,65,46,193,79,89,55,216,185,123,12,
              180,12,71,15,13,196,130,207,43,31,80,250,38,78,161,158,225,135,
              94,193,223,148,163,54,119,251,117,87,116,0,14,201,151,110,5,
              233,140,1,86,114,128,105,124,58,23,109,150,44,41,180,75,181,
              225,108,130,239,76,9,46,148,135,91,156,180,49,72,95,110,43,32,
              159,147,196,90,147,0,227,199,37,225,63,78,85,229,89,193,26,97,
              16,22,25,39,153,0,84,250,167,152,251,171,19,208,62,232,112,93,
              36,180,232,230,213,223,174,110,206,150,243,101,52,71,157,158,
              142,252,220,45,231,10,117,167,15,196,20,182,13,246,134,252,64,
              178,5,61,144,221,68,188,229,32,164,13,241,186,52,137,12,145,
              121,181,74,136,71,18,134,162,10,155,127,128,235,175,122,153,
              232,20,24,239,21,233,186,200,17,188,26,143,207,56,182,94,16,
              216,89,169,20,150,30,62,242,178,115,191,219,95,124,6,170,55,
              197,150,139,192,234,170,16,80,196,99,216,254,17>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2024-10-08T19:52:04.954Z,ns_1@cb.local:<0.331.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.331.0>,menelaus_web}
    started: [{pid,<0.370.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:04.954Z,ns_1@cb.local:<0.329.0>:restartable:start_child:92]Started child process <0.331.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2024-10-08T19:52:04.955Z,ns_1@cb.local:ns_ssl_services_sup<0.307.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.329.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:04.955Z,ns_1@cb.local:<0.329.0>:restartable:loop:65]Restarting child <0.331.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.342.0>,#Ref<0.1492486459.688390148.211089>}
[error_logger:info,2024-10-08T19:52:04.955Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.307.0>},
              {id,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:04.956Z,ns_1@cb.local:<0.329.0>:restartable:shutdown_child:114]Successfully terminated process <0.331.0>
[ns_server:info,2024-10-08T19:52:04.966Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2024-10-08T19:52:04.966Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2024-10-08T19:52:04.967Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2024-10-08T19:52:04.967Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2024-10-08T19:52:04.967Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2024-10-08T19:52:04.969Z,ns_1@cb.local:<0.389.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,23,252,146,55,88,95,
              181,245,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,57,48,54,53,100,56,54,48,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,57,48,54,
              53,100,56,54,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,162,224,138,62,47,
              29,63,50,251,82,88,207,107,131,220,30,65,12,143,185,31,172,113,
              59,22,174,54,239,57,221,161,248,234,29,94,218,78,193,138,220,
              147,199,168,81,58,124,24,29,159,88,41,0,242,149,73,22,245,178,
              114,180,147,20,239,109,133,113,39,44,1,245,32,115,12,103,191,
              191,243,103,66,41,139,224,181,104,75,204,233,190,254,35,202,
              119,119,161,149,233,229,112,254,211,9,64,4,128,202,232,24,168,
              24,255,52,201,191,28,153,207,56,236,83,15,15,179,38,49,16,223,
              57,145,180,191,222,142,136,118,138,41,179,80,34,13,67,166,242,
              138,186,102,149,120,148,184,38,173,189,15,12,141,149,33,172,
              110,178,14,57,167,22,20,187,34,114,177,168,196,187,61,28,151,
              63,173,196,85,194,96,201,47,89,193,132,208,82,40,193,101,124,
              93,134,155,178,23,194,48,21,65,100,197,113,23,53,81,192,121,
              199,175,193,48,89,188,238,196,219,32,96,177,93,13,22,26,184,
              143,95,109,187,229,244,157,164,131,7,48,179,19,116,224,19,216,
              129,216,95,93,133,41,207,84,108,171,188,59,2,3,1,0,1,163,66,48,
              64,48,14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,
              1,1,255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,72,161,1,
              41,120,181,194,241,208,64,236,198,92,76,179,173,233,21,231,254,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,52,52,
              146,106,72,62,32,98,181,69,110,40,169,232,133,53,197,31,60,181,
              173,120,160,236,167,241,41,65,46,193,79,89,55,216,185,123,12,
              180,12,71,15,13,196,130,207,43,31,80,250,38,78,161,158,225,135,
              94,193,223,148,163,54,119,251,117,87,116,0,14,201,151,110,5,
              233,140,1,86,114,128,105,124,58,23,109,150,44,41,180,75,181,
              225,108,130,239,76,9,46,148,135,91,156,180,49,72,95,110,43,32,
              159,147,196,90,147,0,227,199,37,225,63,78,85,229,89,193,26,97,
              16,22,25,39,153,0,84,250,167,152,251,171,19,208,62,232,112,93,
              36,180,232,230,213,223,174,110,206,150,243,101,52,71,157,158,
              142,252,220,45,231,10,117,167,15,196,20,182,13,246,134,252,64,
              178,5,61,144,221,68,188,229,32,164,13,241,186,52,137,12,145,
              121,181,74,136,71,18,134,162,10,155,127,128,235,175,122,153,
              232,20,24,239,21,233,186,200,17,188,26,143,207,56,182,94,16,
              216,89,169,20,150,30,62,242,178,115,191,219,95,124,6,170,55,
              197,150,139,192,234,170,16,80,196,99,216,254,17>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2024-10-08T19:52:04.973Z,ns_1@cb.local:<0.389.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.389.0>,menelaus_web}
    started: [{pid,<0.390.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:04.979Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2024-10-08T19:52:04.979Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[error_logger:info,2024-10-08T19:52:04.980Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.409.0>},
              {id,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:04.980Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2024-10-08T19:52:04.986Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2024-10-08T19:52:04.996Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:debug,2024-10-08T19:52:04.996Z,ns_1@cb.local:cb_saml<0.411.0>:cb_saml:handle_info:316]Settings have changed or this is the first start
[ns_server:debug,2024-10-08T19:52:04.997Z,ns_1@cb.local:cb_saml<0.411.0>:cb_saml:restart_refresh_timer:586]Restarting refresh timer: 7429 ms
[error_logger:info,2024-10-08T19:52:04.996Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.411.0>},
              {id,cb_saml},
              {mfargs,{cb_saml,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:04.998Z,ns_1@cb.local:<0.389.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,23,252,146,55,88,95,
              181,245,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,57,48,54,53,100,56,54,48,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,57,48,54,
              53,100,56,54,48,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,162,224,138,62,47,
              29,63,50,251,82,88,207,107,131,220,30,65,12,143,185,31,172,113,
              59,22,174,54,239,57,221,161,248,234,29,94,218,78,193,138,220,
              147,199,168,81,58,124,24,29,159,88,41,0,242,149,73,22,245,178,
              114,180,147,20,239,109,133,113,39,44,1,245,32,115,12,103,191,
              191,243,103,66,41,139,224,181,104,75,204,233,190,254,35,202,
              119,119,161,149,233,229,112,254,211,9,64,4,128,202,232,24,168,
              24,255,52,201,191,28,153,207,56,236,83,15,15,179,38,49,16,223,
              57,145,180,191,222,142,136,118,138,41,179,80,34,13,67,166,242,
              138,186,102,149,120,148,184,38,173,189,15,12,141,149,33,172,
              110,178,14,57,167,22,20,187,34,114,177,168,196,187,61,28,151,
              63,173,196,85,194,96,201,47,89,193,132,208,82,40,193,101,124,
              93,134,155,178,23,194,48,21,65,100,197,113,23,53,81,192,121,
              199,175,193,48,89,188,238,196,219,32,96,177,93,13,22,26,184,
              143,95,109,187,229,244,157,164,131,7,48,179,19,116,224,19,216,
              129,216,95,93,133,41,207,84,108,171,188,59,2,3,1,0,1,163,66,48,
              64,48,14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,
              1,1,255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,72,161,1,
              41,120,181,194,241,208,64,236,198,92,76,179,173,233,21,231,254,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,52,52,
              146,106,72,62,32,98,181,69,110,40,169,232,133,53,197,31,60,181,
              173,120,160,236,167,241,41,65,46,193,79,89,55,216,185,123,12,
              180,12,71,15,13,196,130,207,43,31,80,250,38,78,161,158,225,135,
              94,193,223,148,163,54,119,251,117,87,116,0,14,201,151,110,5,
              233,140,1,86,114,128,105,124,58,23,109,150,44,41,180,75,181,
              225,108,130,239,76,9,46,148,135,91,156,180,49,72,95,110,43,32,
              159,147,196,90,147,0,227,199,37,225,63,78,85,229,89,193,26,97,
              16,22,25,39,153,0,84,250,167,152,251,171,19,208,62,232,112,93,
              36,180,232,230,213,223,174,110,206,150,243,101,52,71,157,158,
              142,252,220,45,231,10,117,167,15,196,20,182,13,246,134,252,64,
              178,5,61,144,221,68,188,229,32,164,13,241,186,52,137,12,145,
              121,181,74,136,71,18,134,162,10,155,127,128,235,175,122,153,
              232,20,24,239,21,233,186,200,17,188,26,143,207,56,182,94,16,
              216,89,169,20,150,30,62,242,178,115,191,219,95,124,6,170,55,
              197,150,139,192,234,170,16,80,196,99,216,254,17>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2024-10-08T19:52:05.003Z,ns_1@cb.local:<0.389.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.389.0>,menelaus_web}
    started: [{pid,<0.413.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.39330155>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:05.003Z,ns_1@cb.local:<0.329.0>:restartable:start_child:92]Started child process <0.389.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2024-10-08T19:52:05.003Z,ns_1@cb.local:<0.342.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service ssl_service
[ns_server:info,2024-10-08T19:52:05.004Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [ssl_service,server_cert_event,
                               client_cert_event,cb_dist_tls]
[error_logger:info,2024-10-08T19:52:05.009Z,ns_1@cb.local:users_sup<0.432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.433.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:05.027Z,ns_1@cb.local:users_storage_sup<0.434.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.435.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:05.032Z,ns_1@cb.local:users_replicator<0.435.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[ns_server:info,2024-10-08T19:52:05.041Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2024-10-08T19:52:05.041Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[ns_server:debug,2024-10-08T19:52:05.041Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2024-10-08T19:52:05.045Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:announce_startup:61]Announce my startup to <0.435.0>
[ns_server:debug,2024-10-08T19:52:05.046Z,ns_1@cb.local:users_replicator<0.435.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <0.436.0>
[ns_server:debug,2024-10-08T19:52:05.046Z,ns_1@cb.local:users_storage<0.436.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2024-10-08T19:52:05.046Z,ns_1@cb.local:users_storage_sup<0.434.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.436.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:05.046Z,ns_1@cb.local:users_sup<0.432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.434.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:05.055Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2024-10-08T19:52:05.056Z,ns_1@cb.local:<0.445.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2024-10-08T19:52:05.056Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998018,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:05.056Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:05.056Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211408>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:05.056Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211408>,
                                  inet_tcp_dist,<0.446.0>,
                                  #Ref<0.1492486459.688390147.211411>}
[ns_server:debug,2024-10-08T19:52:05.057Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211408>,
                               inet_tcp_dist,<0.446.0>,
                               #Ref<0.1492486459.688390147.211411>}
[error_logger:info,2024-10-08T19:52:05.057Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.446.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:05.058Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:05.058Z,ns_1@cb.local:<0.444.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2024-10-08T19:52:05.058Z,ns_1@cb.local:<0.444.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:debug,2024-10-08T19:52:05.070Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2024-10-08T19:52:05.070Z,ns_1@cb.local:users_sup<0.432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.448.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:05.080Z,ns_1@cb.local:users_sup<0.432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.451.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:05.080Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[error_logger:info,2024-10-08T19:52:05.080Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.432.0>},
              {id,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:05.080Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588799000
[error_logger:info,2024-10-08T19:52:05.083Z,ns_1@cb.local:kernel_safe_sup<0.68.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.455.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:05.083Z,ns_1@cb.local:kernel_safe_sup<0.68.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.456.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:05.083Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2024-10-08T19:52:05.088Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:validate_pkey:1032]Private key (client_cert) passphrase validation suceeded
[ns_server:debug,2024-10-08T19:52:05.089Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2024-10-08T19:52:05.089Z,ns_1@cb.local:<0.463.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2024-10-08T19:52:05.089Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998019,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:05.092Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:05.092Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212335>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:05.093Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212335>,
                                  inet_tcp_dist,<0.465.0>,
                                  #Ref<0.1492486459.688390146.212338>}
[error_logger:info,2024-10-08T19:52:05.095Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.465.0>,shutdown}}
[ns_server:debug,2024-10-08T19:52:05.096Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212335>,
                               inet_tcp_dist,<0.465.0>,
                               #Ref<0.1492486459.688390146.212338>}
[error_logger:info,2024-10-08T19:52:05.096Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:05.096Z,ns_1@cb.local:<0.464.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2024-10-08T19:52:05.096Z,ns_1@cb.local:<0.464.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:debug,2024-10-08T19:52:05.114Z,ns_1@cb.local:<0.469.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    start_couchdb_node} with delay of 5000
[ns_server:debug,2024-10-08T19:52:05.114Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636325}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2024-10-08T19:52:05.115Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636325}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:info,2024-10-08T19:52:05.138Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[ns_server:debug,2024-10-08T19:52:05.201Z,ns_1@cb.local:users_storage<0.436.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2024-10-08T19:52:05.201Z,ns_1@cb.local:users_storage<0.436.0>:replicated_dets:init_after_ack:170]Loading 0 items, 305 words took 155ms
[error_logger:info,2024-10-08T19:52:05.217Z,ns_1@cb.local:<0.470.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.470.0>,suppress_max_restart_intensity}
    started: [{pid,<0.471.0>},
              {id,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:05.218Z,ns_1@cb.local:<0.468.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.468.0>,suppress_max_restart_intensity}
    started: [{pid,<0.469.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      start_couchdb_node}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,start_couchdb_node},
                       5000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 5,id => start_couchdb_node,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => 86400000,
                          start => {ns_server_nodes_sup,start_couchdb_node,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:05.219Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.468.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,start_couchdb_node}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 5,id => start_couchdb_node,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => 86400000,
                         start => {ns_server_nodes_sup,start_couchdb_node,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:05.219Z,ns_1@cb.local:wait_link_to_couchdb_node<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[error_logger:info,2024-10-08T19:52:05.220Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998020,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:05.220Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:05.220Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212377>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:05.221Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212377>,
                                  inet_tcp_dist,<0.475.0>,
                                  #Ref<0.1492486459.688390146.212380>}
[error_logger:info,2024-10-08T19:52:05.222Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.475.0>,shutdown}}
[ns_server:debug,2024-10-08T19:52:05.222Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212377>,
                               inet_tcp_dist,<0.475.0>,
                               #Ref<0.1492486459.688390146.212380>}
[error_logger:info,2024-10-08T19:52:05.222Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:05.222Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:05.423Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998021,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:05.424Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:05.424Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212382>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:05.425Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212382>,
                                  inet_tcp_dist,<0.477.0>,
                                  #Ref<0.1492486459.688390146.212385>}
[ns_server:debug,2024-10-08T19:52:05.426Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212382>,
                               inet_tcp_dist,<0.477.0>,
                               #Ref<0.1492486459.688390146.212385>}
[error_logger:info,2024-10-08T19:52:05.427Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.477.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:05.427Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:05.427Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:05.629Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998022,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:05.630Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:05.630Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212394>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:05.630Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212394>,
                                  inet_tcp_dist,<0.479.0>,
                                  #Ref<0.1492486459.688390145.211991>}
[ns_server:debug,2024-10-08T19:52:05.632Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212394>,
                               inet_tcp_dist,<0.479.0>,
                               #Ref<0.1492486459.688390145.211991>}
[error_logger:info,2024-10-08T19:52:05.632Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.479.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:05.632Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:05.632Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:05.833Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998023,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:05.834Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:05.834Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212403>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:05.834Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212403>,
                                  inet_tcp_dist,<0.481.0>,
                                  #Ref<0.1492486459.688390146.212406>}
[error_logger:info,2024-10-08T19:52:05.835Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.481.0>,shutdown}}
[ns_server:debug,2024-10-08T19:52:05.835Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212403>,
                               inet_tcp_dist,<0.481.0>,
                               #Ref<0.1492486459.688390146.212406>}
[error_logger:info,2024-10-08T19:52:05.836Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:05.836Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:06.038Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998024,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:06.038Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:06.039Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212412>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:06.039Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212412>,
                                  inet_tcp_dist,<0.483.0>,
                                  #Ref<0.1492486459.688390146.212415>}
[error_logger:info,2024-10-08T19:52:06.040Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.483.0>,shutdown}}
[ns_server:debug,2024-10-08T19:52:06.040Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212412>,
                               inet_tcp_dist,<0.483.0>,
                               #Ref<0.1492486459.688390146.212415>}
[error_logger:info,2024-10-08T19:52:06.040Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:06.041Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:06.242Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998025,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:06.243Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:06.244Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211183>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:06.244Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211183>,
                                  inet_tcp_dist,<0.485.0>,
                                  #Ref<0.1492486459.688390148.211185>}
[ns_server:debug,2024-10-08T19:52:06.245Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211183>,
                               inet_tcp_dist,<0.485.0>,
                               #Ref<0.1492486459.688390148.211185>}
[error_logger:info,2024-10-08T19:52:06.245Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.485.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:06.246Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:06.246Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:06.447Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998026,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:06.448Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:06.448Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212012>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:06.448Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212012>,
                                  inet_tcp_dist,<0.487.0>,
                                  #Ref<0.1492486459.688390148.211188>}
[ns_server:debug,2024-10-08T19:52:06.449Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212012>,
                               inet_tcp_dist,<0.487.0>,
                               #Ref<0.1492486459.688390148.211188>}
[error_logger:info,2024-10-08T19:52:06.449Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.487.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:06.449Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:06.449Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:06.650Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998027,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:06.650Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:06.651Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212024>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:06.651Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212024>,
                                  inet_tcp_dist,<0.489.0>,
                                  #Ref<0.1492486459.688390145.212027>}
[ns_server:debug,2024-10-08T19:52:06.652Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212024>,
                               inet_tcp_dist,<0.489.0>,
                               #Ref<0.1492486459.688390145.212027>}
[error_logger:info,2024-10-08T19:52:06.653Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.489.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:06.653Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:06.653Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:06.854Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998028,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:06.855Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:06.855Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211194>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:06.856Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211194>,
                                  inet_tcp_dist,<0.491.0>,
                                  #Ref<0.1492486459.688390148.211196>}
[error_logger:info,2024-10-08T19:52:06.856Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.491.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:06.857Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:06.857Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211194>,
                               inet_tcp_dist,<0.491.0>,
                               #Ref<0.1492486459.688390148.211196>}
[ns_server:debug,2024-10-08T19:52:06.857Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:07.058Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998029,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:07.058Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:07.059Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211203>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:07.059Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211203>,
                                  inet_tcp_dist,<0.493.0>,
                                  #Ref<0.1492486459.688390148.211206>}
[ns_server:debug,2024-10-08T19:52:07.059Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211203>,
                               inet_tcp_dist,<0.493.0>,
                               #Ref<0.1492486459.688390148.211206>}
[error_logger:info,2024-10-08T19:52:07.060Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.493.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:07.060Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:07.060Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:07.261Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998030,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:07.262Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:07.262Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211525>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:07.262Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211525>,
                                  inet_tcp_dist,<0.495.0>,
                                  #Ref<0.1492486459.688390147.211528>}
[ns_server:debug,2024-10-08T19:52:07.263Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211525>,
                               inet_tcp_dist,<0.495.0>,
                               #Ref<0.1492486459.688390147.211528>}
[error_logger:info,2024-10-08T19:52:07.263Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.495.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:07.263Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:07.264Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:07.464Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998031,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:07.464Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:07.464Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212431>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:07.465Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212431>,
                                  inet_tcp_dist,<0.497.0>,
                                  #Ref<0.1492486459.688390146.212434>}
[error_logger:info,2024-10-08T19:52:07.466Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.497.0>,shutdown}}
[error_logger:info,2024-10-08T19:52:07.466Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:07.466Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:07.466Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212431>,
                               inet_tcp_dist,<0.497.0>,
                               #Ref<0.1492486459.688390146.212434>}
[error_logger:info,2024-10-08T19:52:07.672Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998032,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:07.674Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:07.674Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212039>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:07.675Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212039>,
                                  inet_tcp_dist,<0.499.0>,
                                  #Ref<0.1492486459.688390148.211217>}
[ns_server:debug,2024-10-08T19:52:07.947Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212039>,
                               inet_tcp_dist,<0.499.0>,
                               #Ref<0.1492486459.688390148.211217>}
[error_logger:info,2024-10-08T19:52:07.947Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.499.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:07.948Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:07.948Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:08.148Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998033,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:08.149Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:08.149Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212045>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:08.149Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212045>,
                                  inet_tcp_dist,<0.501.0>,
                                  #Ref<0.1492486459.688390148.211227>}
[ns_server:info,2024-10-08T19:52:08.180Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: =ERROR REPORT==== 8-Oct-2024::19:52:07.946113 ===
ns_couchdb<0.471.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.471.0>: 

[ns_server:debug,2024-10-08T19:52:08.184Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212045>,
                               inet_tcp_dist,<0.501.0>,
                               #Ref<0.1492486459.688390148.211227>}
[error_logger:info,2024-10-08T19:52:08.184Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.501.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:08.185Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:08.185Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:info,2024-10-08T19:52:08.384Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: =ERROR REPORT==== 8-Oct-2024::19:52:08.183433 ===
ns_couchdb<0.471.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.471.0>: 

[error_logger:info,2024-10-08T19:52:08.385Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998034,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:08.386Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:08.386Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211233>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:08.386Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211233>,
                                  inet_tcp_dist,<0.503.0>,
                                  #Ref<0.1492486459.688390148.211236>}
[ns_server:debug,2024-10-08T19:52:08.416Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211233>,
                               inet_tcp_dist,<0.503.0>,
                               #Ref<0.1492486459.688390148.211236>}
[error_logger:info,2024-10-08T19:52:08.416Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.503.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:08.416Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:08.416Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:info,2024-10-08T19:52:08.617Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: =ERROR REPORT==== 8-Oct-2024::19:52:08.413372 ===
ns_couchdb<0.471.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.471.0>: 

[error_logger:info,2024-10-08T19:52:08.617Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998035,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:08.618Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:08.618Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211242>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:08.618Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211242>,
                                  inet_tcp_dist,<0.505.0>,
                                  #Ref<0.1492486459.688390148.211245>}
[ns_server:debug,2024-10-08T19:52:08.645Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211242>,
                               inet_tcp_dist,<0.505.0>,
                               #Ref<0.1492486459.688390148.211245>}
[error_logger:info,2024-10-08T19:52:08.645Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.505.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:08.645Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:08.645Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:08.846Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998036,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:08.847Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:08.847Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211254>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:08.847Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211254>,
                                  inet_tcp_dist,<0.507.0>,
                                  #Ref<0.1492486459.688390147.211568>}
[ns_server:debug,2024-10-08T19:52:08.851Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211254>,
                               inet_tcp_dist,<0.507.0>,
                               #Ref<0.1492486459.688390147.211568>}
[error_logger:info,2024-10-08T19:52:08.851Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.507.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:08.851Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:08.851Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:09.052Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998037,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:09.053Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:09.053Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211575>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:09.053Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211575>,
                                  inet_tcp_dist,<0.509.0>,
                                  #Ref<0.1492486459.688390147.211578>}
[error_logger:info,2024-10-08T19:52:09.055Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.509.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2024-10-08T19:52:09.055Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211575>,
                               inet_tcp_dist,<0.509.0>,
                               #Ref<0.1492486459.688390147.211578>}
[ns_server:debug,2024-10-08T19:52:09.056Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:09.056Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2024-10-08T19:52:09.257Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998038,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:09.258Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:09.258Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211273>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:09.258Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211273>,
                                  inet_tcp_dist,<0.511.0>,
                                  #Ref<0.1492486459.688390147.211583>}
[ns_server:debug,2024-10-08T19:52:09.267Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211273>,
                               inet_tcp_dist,<0.511.0>,
                               #Ref<0.1492486459.688390147.211583>}
[error_logger:info,2024-10-08T19:52:09.267Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.511.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:09.267Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:09.267Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:09.468Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998039,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:09.469Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:09.469Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211281>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:09.469Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211281>,
                                  inet_tcp_dist,<0.513.0>,
                                  #Ref<0.1492486459.688390148.211284>}
[ns_server:debug,2024-10-08T19:52:09.475Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211281>,
                               inet_tcp_dist,<0.513.0>,
                               #Ref<0.1492486459.688390148.211284>}
[error_logger:info,2024-10-08T19:52:09.475Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.513.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:09.476Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:09.476Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:09.677Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998040,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:09.678Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:09.678Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211290>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:09.678Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211290>,
                                  inet_tcp_dist,<0.515.0>,
                                  #Ref<0.1492486459.688390148.211293>}
[error_logger:info,2024-10-08T19:52:09.683Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.515.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2024-10-08T19:52:09.683Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211290>,
                               inet_tcp_dist,<0.515.0>,
                               #Ref<0.1492486459.688390148.211293>}
[error_logger:info,2024-10-08T19:52:09.683Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:09.684Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:09.884Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998041,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:09.885Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:09.885Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211300>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:09.885Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211300>,
                                  inet_tcp_dist,<0.517.0>,
                                  #Ref<0.1492486459.688390148.211303>}
[ns_server:debug,2024-10-08T19:52:09.897Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211300>,
                               inet_tcp_dist,<0.517.0>,
                               #Ref<0.1492486459.688390148.211303>}
[error_logger:info,2024-10-08T19:52:09.897Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.517.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:09.897Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:09.897Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:10.055Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2024-10-08T19:52:10.055Z,ns_1@cb.local:<0.524.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2024-10-08T19:52:10.056Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998042,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:10.056Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:10.056Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211602>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:10.057Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211602>,
                                  inet_tcp_dist,<0.525.0>,
                                  #Ref<0.1492486459.688390147.211604>}
[ns_server:debug,2024-10-08T19:52:10.130Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211602>,
                               inet_tcp_dist,<0.525.0>,
                               #Ref<0.1492486459.688390147.211604>}
[error_logger:info,2024-10-08T19:52:10.130Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.525.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:10.130Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:10.131Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:10.131Z,ns_1@cb.local:<0.523.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2024-10-08T19:52:10.131Z,ns_1@cb.local:<0.523.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2024-10-08T19:52:10.178Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2024-10-08T19:52:10.178Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2024-10-08T19:52:10.178Z,ns_1@cb.local:<0.531.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2024-10-08T19:52:10.178Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998043,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:10.179Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:10.179Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212079>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:10.179Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212079>,
                                  inet_tcp_dist,<0.533.0>,
                                  #Ref<0.1492486459.688390146.212464>}
[ns_server:debug,2024-10-08T19:52:10.182Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212079>,
                               inet_tcp_dist,<0.533.0>,
                               #Ref<0.1492486459.688390146.212464>}
[error_logger:info,2024-10-08T19:52:10.182Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.533.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:10.182Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:10.182Z,ns_1@cb.local:<0.532.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2024-10-08T19:52:10.189Z,ns_1@cb.local:<0.532.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2024-10-08T19:52:10.231Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[error_logger:info,2024-10-08T19:52:10.331Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998044,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:10.332Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:10.332Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212480>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:10.332Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212480>,
                                  inet_tcp_dist,<0.535.0>,
                                  #Ref<0.1492486459.688390146.212482>}
[error_logger:info,2024-10-08T19:52:10.349Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.535.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:10.349Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:10.349Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:10.349Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212480>,
                               inet_tcp_dist,<0.535.0>,
                               #Ref<0.1492486459.688390146.212482>}
[error_logger:info,2024-10-08T19:52:10.550Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998045,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:10.551Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:10.551Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212090>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:10.551Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212090>,
                                  inet_tcp_dist,<0.537.0>,
                                  #Ref<0.1492486459.688390145.212093>}
[error_logger:info,2024-10-08T19:52:10.553Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.537.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2024-10-08T19:52:10.553Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212090>,
                               inet_tcp_dist,<0.537.0>,
                               #Ref<0.1492486459.688390145.212093>}
[error_logger:info,2024-10-08T19:52:10.553Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:10.553Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:10.756Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998046,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:10.756Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:10.757Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211320>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:10.757Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211320>,
                                  inet_tcp_dist,<0.539.0>,
                                  #Ref<0.1492486459.688390148.211323>}
[error_logger:info,2024-10-08T19:52:10.773Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.539.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:10.773Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:10.774Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211320>,
                               inet_tcp_dist,<0.539.0>,
                               #Ref<0.1492486459.688390148.211323>}
[ns_server:debug,2024-10-08T19:52:10.774Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:10.974Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998047,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:10.975Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:10.975Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211647>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:10.975Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211647>,
                                  inet_tcp_dist,<0.541.0>,
                                  #Ref<0.1492486459.688390147.211650>}
[error_logger:info,2024-10-08T19:52:10.977Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.541.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2024-10-08T19:52:10.977Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211647>,
                               inet_tcp_dist,<0.541.0>,
                               #Ref<0.1492486459.688390147.211650>}
[ns_server:debug,2024-10-08T19:52:10.977Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:10.977Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2024-10-08T19:52:11.178Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998048,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:11.179Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:11.179Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212489>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:11.179Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212489>,
                                  inet_tcp_dist,<0.543.0>,
                                  #Ref<0.1492486459.688390146.212491>}
[ns_server:debug,2024-10-08T19:52:11.181Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212489>,
                               inet_tcp_dist,<0.543.0>,
                               #Ref<0.1492486459.688390146.212491>}
[error_logger:info,2024-10-08T19:52:11.181Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.543.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:11.182Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:11.182Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:11.383Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998049,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:11.384Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:11.384Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212102>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:11.384Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212102>,
                                  inet_tcp_dist,<0.545.0>,
                                  #Ref<0.1492486459.688390147.211662>}
[error_logger:info,2024-10-08T19:52:11.899Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.545.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:11.899Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:11.899Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212102>,
                               inet_tcp_dist,<0.545.0>,
                               #Ref<0.1492486459.688390147.211662>}
[ns_server:debug,2024-10-08T19:52:11.899Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:12.729Z,ns_1@cb.local:cb_saml<0.411.0>:cb_saml:refresh_metadata:526]Refreshing metadata
[error_logger:info,2024-10-08T19:52:12.729Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998050,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:12.729Z,ns_1@cb.local:cb_saml<0.411.0>:cb_saml:restart_refresh_timer:583]Disabling refresh timer
[ns_server:debug,2024-10-08T19:52:12.729Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:12.730Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211345>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:12.730Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211345>,
                                  inet_tcp_dist,<0.547.0>,
                                  #Ref<0.1492486459.688390148.211348>}
[error_logger:info,2024-10-08T19:52:13.665Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.547.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:13.665Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:13.665Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211345>,
                               inet_tcp_dist,<0.547.0>,
                               #Ref<0.1492486459.688390148.211348>}
[ns_server:debug,2024-10-08T19:52:13.665Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:13.866Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998051,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:13.968Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:13.968Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211356>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:13.968Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211356>,
                                  inet_tcp_dist,<0.549.0>,
                                  #Ref<0.1492486459.688390147.211710>}
[error_logger:info,2024-10-08T19:52:14.178Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.549.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2024-10-08T19:52:14.177Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211356>,
                               inet_tcp_dist,<0.549.0>,
                               #Ref<0.1492486459.688390147.211710>}
[error_logger:info,2024-10-08T19:52:14.178Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:14.178Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:14.475Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998052,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:14.475Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:14.475Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211371>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:14.475Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211371>,
                                  inet_tcp_dist,<0.551.0>,
                                  #Ref<0.1492486459.688390148.211374>}
[ns_server:debug,2024-10-08T19:52:14.732Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211371>,
                               inet_tcp_dist,<0.551.0>,
                               #Ref<0.1492486459.688390148.211374>}
[error_logger:info,2024-10-08T19:52:14.732Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.551.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:14.733Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:14.733Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:14.941Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998053,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:14.941Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:14.941Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211387>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:14.942Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211387>,
                                  inet_tcp_dist,<0.553.0>,
                                  #Ref<0.1492486459.688390148.211390>}
[error_logger:info,2024-10-08T19:52:15.122Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.553.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2024-10-08T19:52:15.122Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211387>,
                               inet_tcp_dist,<0.553.0>,
                               #Ref<0.1492486459.688390148.211390>}
[ns_server:debug,2024-10-08T19:52:15.123Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:15.122Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:15.183Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2024-10-08T19:52:15.184Z,ns_1@cb.local:<0.592.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2024-10-08T19:52:15.184Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998054,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:15.184Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:15.184Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212523>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:15.184Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212523>,
                                  inet_tcp_dist,<0.593.0>,
                                  #Ref<0.1492486459.688390146.212526>}
[error_logger:info,2024-10-08T19:52:15.272Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.593.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2024-10-08T19:52:15.272Z,ns_1@cb.local:<0.591.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:debug,2024-10-08T19:52:15.272Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212523>,
                               inet_tcp_dist,<0.593.0>,
                               #Ref<0.1492486459.688390146.212526>}
[ns_server:warn,2024-10-08T19:52:15.273Z,ns_1@cb.local:<0.591.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[error_logger:info,2024-10-08T19:52:15.272Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2024-10-08T19:52:15.323Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998055,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:15.323Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:15.324Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212145>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:15.324Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212145>,
                                  inet_tcp_dist,<0.595.0>,
                                  #Ref<0.1492486459.688390145.212148>}
[error_logger:info,2024-10-08T19:52:15.335Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.595.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:15.335Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:15.336Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212145>,
                               inet_tcp_dist,<0.595.0>,
                               #Ref<0.1492486459.688390145.212148>}
[ns_server:debug,2024-10-08T19:52:15.336Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:15.537Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998056,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:15.538Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:15.538Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212159>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:15.538Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212159>,
                                  inet_tcp_dist,<0.597.0>,
                                  #Ref<0.1492486459.688390145.212162>}
[ns_server:debug,2024-10-08T19:52:16.121Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212159>,
                               inet_tcp_dist,<0.597.0>,
                               #Ref<0.1492486459.688390145.212162>}
[error_logger:info,2024-10-08T19:52:16.135Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.597.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:16.136Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:16.136Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:16.387Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998057,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:16.388Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:16.388Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212538>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:16.388Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212538>,
                                  inet_tcp_dist,<0.599.0>,
                                  #Ref<0.1492486459.688390146.212541>}
[error_logger:info,2024-10-08T19:52:16.439Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.599.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:16.439Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:16.440Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:16.440Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212538>,
                               inet_tcp_dist,<0.599.0>,
                               #Ref<0.1492486459.688390146.212541>}
[error_logger:info,2024-10-08T19:52:16.667Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998058,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:16.667Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:16.667Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212168>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:16.667Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212168>,
                                  inet_tcp_dist,<0.601.0>,
                                  #Ref<0.1492486459.688390145.212171>}
[error_logger:info,2024-10-08T19:52:16.678Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.601.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:16.678Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:16.678Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212168>,
                               inet_tcp_dist,<0.601.0>,
                               #Ref<0.1492486459.688390145.212171>}
[ns_server:debug,2024-10-08T19:52:16.678Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:16.888Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998059,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:16.888Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:16.888Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212182>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:16.888Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212182>,
                                  inet_tcp_dist,<0.603.0>,
                                  #Ref<0.1492486459.688390145.212185>}
[ns_server:info,2024-10-08T19:52:16.866Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[error_logger:info,2024-10-08T19:52:16.916Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.603.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:16.916Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:16.916Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212182>,
                               inet_tcp_dist,<0.603.0>,
                               #Ref<0.1492486459.688390145.212185>}
[ns_server:debug,2024-10-08T19:52:16.916Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:16.898Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2024-10-08T19:52:16.917Z,ns_1@cb.local:<0.609.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2024-10-08T19:52:16.917Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998060,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:16.918Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:16.918Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212203>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:16.918Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212203>,
                                  inet_tcp_dist,<0.611.0>,
                                  #Ref<0.1492486459.688390145.212206>}
[ns_server:debug,2024-10-08T19:52:16.922Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212203>,
                               inet_tcp_dist,<0.611.0>,
                               #Ref<0.1492486459.688390145.212206>}
[error_logger:info,2024-10-08T19:52:16.922Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.611.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:16.933Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:16.934Z,ns_1@cb.local:<0.610.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1203}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1179}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2024-10-08T19:52:16.934Z,ns_1@cb.local:<0.610.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[error_logger:info,2024-10-08T19:52:17.127Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998061,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:17.127Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:17.127Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212222>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:17.127Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212222>,
                                  inet_tcp_dist,<0.613.0>,
                                  #Ref<0.1492486459.688390145.212225>}
[error_logger:info,2024-10-08T19:52:17.140Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.613.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:17.140Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:17.140Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:17.140Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212222>,
                               inet_tcp_dist,<0.613.0>,
                               #Ref<0.1492486459.688390145.212225>}
[ns_server:info,2024-10-08T19:52:17.190Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[error_logger:info,2024-10-08T19:52:17.341Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998062,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:17.342Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:17.342Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211410>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:17.342Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211410>,
                                  inet_tcp_dist,<0.615.0>,
                                  #Ref<0.1492486459.688390148.211413>}
[error_logger:info,2024-10-08T19:52:17.344Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.615.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:17.344Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:17.344Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:17.344Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211410>,
                               inet_tcp_dist,<0.615.0>,
                               #Ref<0.1492486459.688390148.211413>}
[error_logger:info,2024-10-08T19:52:17.545Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998063,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:17.546Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:17.546Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211786>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:17.546Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211786>,
                                  inet_tcp_dist,<0.617.0>,
                                  #Ref<0.1492486459.688390148.211422>}
[ns_server:debug,2024-10-08T19:52:17.548Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211786>,
                               inet_tcp_dist,<0.617.0>,
                               #Ref<0.1492486459.688390148.211422>}
[error_logger:info,2024-10-08T19:52:17.548Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.617.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:17.548Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:17.548Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:17.749Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998064,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:17.749Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:17.750Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212555>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:17.750Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212555>,
                                  inet_tcp_dist,<0.619.0>,
                                  #Ref<0.1492486459.688390145.212232>}
[ns_server:debug,2024-10-08T19:52:17.752Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212555>,
                               inet_tcp_dist,<0.619.0>,
                               #Ref<0.1492486459.688390145.212232>}
[error_logger:info,2024-10-08T19:52:17.752Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.619.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:17.752Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:17.752Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:17.953Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998065,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:17.954Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:17.954Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211426>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:17.954Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211426>,
                                  inet_tcp_dist,<0.621.0>,
                                  #Ref<0.1492486459.688390148.211428>}
[error_logger:info,2024-10-08T19:52:17.961Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.621.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:17.961Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:17.961Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:17.961Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211426>,
                               inet_tcp_dist,<0.621.0>,
                               #Ref<0.1492486459.688390148.211428>}
[error_logger:info,2024-10-08T19:52:18.161Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998066,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:18.161Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:18.161Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211807>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:18.162Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211807>,
                                  inet_tcp_dist,<0.623.0>,
                                  #Ref<0.1492486459.688390147.211810>}
[error_logger:info,2024-10-08T19:52:18.164Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.623.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:18.164Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:18.165Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2024-10-08T19:52:18.165Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211807>,
                               inet_tcp_dist,<0.623.0>,
                               #Ref<0.1492486459.688390147.211810>}
[error_logger:info,2024-10-08T19:52:18.366Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998067,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:18.366Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:18.367Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211822>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:18.367Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211822>,
                                  inet_tcp_dist,<0.625.0>,
                                  #Ref<0.1492486459.688390147.211825>}
[error_logger:info,2024-10-08T19:52:18.368Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.625.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:18.369Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:18.369Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211822>,
                               inet_tcp_dist,<0.625.0>,
                               #Ref<0.1492486459.688390147.211825>}
[ns_server:debug,2024-10-08T19:52:18.369Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:18.570Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998068,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:18.570Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:18.570Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212252>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:18.571Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212252>,
                                  inet_tcp_dist,<0.627.0>,
                                  #Ref<0.1492486459.688390146.212561>}
[error_logger:info,2024-10-08T19:52:18.601Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.627.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:18.601Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:18.601Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212252>,
                               inet_tcp_dist,<0.627.0>,
                               #Ref<0.1492486459.688390146.212561>}
[ns_server:debug,2024-10-08T19:52:18.601Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:18.802Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998069,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:18.802Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:18.803Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212571>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:18.803Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212571>,
                                  inet_tcp_dist,<0.629.0>,
                                  #Ref<0.1492486459.688390146.212574>}
[error_logger:info,2024-10-08T19:52:18.806Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.629.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:18.806Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:18.806Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212571>,
                               inet_tcp_dist,<0.629.0>,
                               #Ref<0.1492486459.688390146.212574>}
[ns_server:debug,2024-10-08T19:52:18.806Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:19.007Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998070,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:19.007Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:19.008Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211844>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:19.008Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211844>,
                                  inet_tcp_dist,<0.631.0>,
                                  #Ref<0.1492486459.688390147.211847>}
[ns_server:debug,2024-10-08T19:52:19.009Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211844>,
                               inet_tcp_dist,<0.631.0>,
                               #Ref<0.1492486459.688390147.211847>}
[error_logger:info,2024-10-08T19:52:19.009Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.631.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:19.011Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:19.011Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:19.212Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998071,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:19.213Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:19.214Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390147.211860>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:19.215Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390147.211860>,
                                  inet_tcp_dist,<0.633.0>,
                                  #Ref<0.1492486459.688390147.211863>}
[ns_server:debug,2024-10-08T19:52:19.219Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390147.211860>,
                               inet_tcp_dist,<0.633.0>,
                               #Ref<0.1492486459.688390147.211863>}
[error_logger:info,2024-10-08T19:52:19.219Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.633.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:19.219Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:19.219Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:19.422Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998072,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:19.422Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:19.422Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212583>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:19.422Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212583>,
                                  inet_tcp_dist,<0.635.0>,
                                  #Ref<0.1492486459.688390146.212586>}
[error_logger:info,2024-10-08T19:52:19.431Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.635.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:19.431Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:19.432Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212583>,
                               inet_tcp_dist,<0.635.0>,
                               #Ref<0.1492486459.688390146.212586>}
[ns_server:debug,2024-10-08T19:52:19.432Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:19.633Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998073,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:19.633Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:19.633Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212601>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:19.634Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212601>,
                                  inet_tcp_dist,<0.637.0>,
                                  #Ref<0.1492486459.688390146.212604>}
[error_logger:info,2024-10-08T19:52:19.636Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.637.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:19.636Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:19.636Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390146.212601>,
                               inet_tcp_dist,<0.637.0>,
                               #Ref<0.1492486459.688390146.212604>}
[ns_server:debug,2024-10-08T19:52:19.636Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:19.845Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998074,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:19.846Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:19.846Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390145.212265>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:19.846Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390145.212265>,
                                  inet_tcp_dist,<0.639.0>,
                                  #Ref<0.1492486459.688390145.212268>}
[error_logger:info,2024-10-08T19:52:19.848Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.639.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:19.848Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:19.848Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390145.212265>,
                               inet_tcp_dist,<0.639.0>,
                               #Ref<0.1492486459.688390145.212268>}
[ns_server:debug,2024-10-08T19:52:19.848Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:20.049Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998075,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:20.049Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:20.049Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211441>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:20.050Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211441>,
                                  inet_tcp_dist,<0.641.0>,
                                  #Ref<0.1492486459.688390148.211444>}
[error_logger:info,2024-10-08T19:52:20.051Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.641.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:20.052Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:20.052Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211441>,
                               inet_tcp_dist,<0.641.0>,
                               #Ref<0.1492486459.688390148.211444>}
[ns_server:debug,2024-10-08T19:52:20.052Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:20.253Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998076,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:20.253Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:20.254Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211450>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:20.254Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211450>,
                                  inet_tcp_dist,<0.643.0>,
                                  #Ref<0.1492486459.688390148.211453>}
[error_logger:info,2024-10-08T19:52:20.257Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.643.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2024-10-08T19:52:20.257Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211450>,
                               inet_tcp_dist,<0.643.0>,
                               #Ref<0.1492486459.688390148.211453>}
[error_logger:info,2024-10-08T19:52:20.257Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:20.257Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:20.459Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998077,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:20.459Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:20.460Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390148.211460>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:20.460Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390148.211460>,
                                  inet_tcp_dist,<0.645.0>,
                                  #Ref<0.1492486459.688390148.211462>}
[ns_server:debug,2024-10-08T19:52:20.462Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.1492486459.688390148.211460>,
                               inet_tcp_dist,<0.645.0>,
                               #Ref<0.1492486459.688390148.211462>}
[error_logger:info,2024-10-08T19:52:20.462Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.645.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2024-10-08T19:52:20.462Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2024-10-08T19:52:20.462Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2024-10-08T19:52:20.663Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {998078,#Ref<0.1492486459.688521217.211863>}}}
[ns_server:debug,2024-10-08T19:52:20.664Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2024-10-08T19:52:20.664Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.1492486459.688390146.212609>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2024-10-08T19:52:20.664Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.1492486459.688390146.212609>,
                                  inet_tcp_dist,<0.647.0>,
                                  #Ref<0.1492486459.688390146.212612>}
[ns_server:debug,2024-10-08T19:52:20.670Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: false
[ns_server:info,2024-10-08T19:52:21.572Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: Apache CouchDB v4.5.1-330-g3e5b8f24 (LogLevel=info) is starting.

[ns_server:debug,2024-10-08T19:52:21.898Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2024-10-08T19:52:21.899Z,ns_1@cb.local:<0.654.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2024-10-08T19:52:22.283Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2024-10-08T19:52:22.778Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.473.0>},
              {id,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.120652322>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:22.800Z,ns_1@cb.local:<0.660.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_disksup} with delay of 4000
[ns_server:info,2024-10-08T19:52:22.817Z,ns_1@cb.local:<0.653.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service capi_ssl_service
[ns_server:info,2024-10-08T19:52:22.818Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [capi_ssl_service]
[error_logger:info,2024-10-08T19:52:22.825Z,ns_1@cb.local:<0.661.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.661.0>,suppress_max_restart_intensity}
    started: [{pid,<0.662.0>},
              {id,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:22.826Z,ns_1@cb.local:<0.659.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.659.0>,suppress_max_restart_intensity}
    started: [{pid,<0.660.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_disksup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_disksup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_disksup,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:22.831Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.659.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_disksup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_disksup,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:22.845Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.663.0>},
              {id,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:22.850Z,ns_1@cb.local:ns_server_sup<0.658.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2024-10-08T19:52:22.852Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.664.0>},
              {id,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:22.859Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.665.0>},
              {id,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:22.865Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.666.0>},
              {id,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2024-10-08T19:52:22.907Z,ns_1@cb.local:ns_log<0.668.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2024-10-08T19:52:22.907Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.668.0>},
              {id,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:22.907Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.669.0>},
              {id,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:22.910Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached}]
[ns_server:debug,2024-10-08T19:52:22.912Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached]
[ns_server:warn,2024-10-08T19:52:22.913Z,ns_1@cb.local:<0.673.0>:ns_ssl_services_setup:notify_service:1184]Failed to notify service memcached: {error,no_proccess}
[ns_server:warn,2024-10-08T19:52:22.921Z,ns_1@cb.local:event_log_server<0.674.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/event_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2024-10-08T19:52:22.921Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.674.0>},
              {id,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:22.948Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1173]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached}]
[ns_server:info,2024-10-08T19:52:22.979Z,ns_1@cb.local:ns_couchdb_port<0.471.0>:ns_port_server:log:226]ns_couchdb<0.471.0>: 198: Booted. Waiting for shutdown request
ns_couchdb<0.471.0>: 198: Booted. Waiting for shutdown request
ns_couchdb<0.471.0>: working as port

[error_logger:info,2024-10-08T19:52:23.089Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.676.0>},
              {id,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:23.093Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.678.0>},
              {id,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:23.093Z,ns_1@cb.local:<0.680.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    ns_babysitter_log_consumer} with delay of 4000
[error_logger:info,2024-10-08T19:52:23.093Z,ns_1@cb.local:<0.681.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.681.0>,suppress_max_restart_intensity}
    started: [{pid,<0.682.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:23.094Z,ns_1@cb.local:<0.679.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.679.0>,suppress_max_restart_intensity}
    started: [{pid,<0.680.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,
                           ns_babysitter_log_consumer},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_babysitter_log_consumer,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start =>
                              {ns_log,start_link_babysitter_log_consumer,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:23.094Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.679.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_babysitter_log_consumer,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start =>
                             {ns_log,start_link_babysitter_log_consumer,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:23.439Z,ns_1@cb.local:<0.687.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2024-10-08T19:52:23.439Z,ns_1@cb.local:<0.687.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2024-10-08T19:52:23.440Z,ns_1@cb.local:<0.687.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2024-10-08T19:52:23.485Z,ns_1@cb.local:prometheus_cfg<0.683.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2024-10-08T19:52:23.540Z,ns_1@cb.local:prometheus_cfg<0.683.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2024-10-08T19:52:23.641Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636343}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"e4G08Sna1TsJej4VN57trjuBU74uTefae3TIG69MEUI=">>}},
    {<<"sha512">>,
     {[{<<"s">>,
        <<"p51YJBmJ60D0LrMhoRGpEPR9Hkw7Xnu9lC5CP1W81r3xWSgnbRZw5tGy9zekreHj+Zs63dX9M/mLBYd32gnPuw==">>},
       {<<"h">>,
        {sanitized,<<"3UfTc9UQQPGSBHaUWM736dVdES0KEaf2jn0Fm7zkpRk=">>}},
       {<<"i">>,15000}]}},
    {<<"sha256">>,
     {[{<<"s">>,<<"aG6ScB6FxjX1sIPlpfEcQRELMMSEyV9wpAqc+mnSNro=">>},
       {<<"h">>,
        {sanitized,<<"BABRopvU8LvGgtsrOP8Okf6afI899phgmBxb2eTUOsU=">>}},
       {<<"i">>,15000}]}},
    {<<"sha1">>,
     {[{<<"s">>,<<"W6VO7N/r14sh+l7ftDS/qgMqeeA=">>},
       {<<"h">>,
        {sanitized,<<"04Sy4JO6CKfM25GI5ijyYoeU3IqItXxXGGQEQtdl9tU=">>}},
       {<<"i">>,15000}]}}]}}]
[ns_server:debug,2024-10-08T19:52:23.721Z,ns_1@cb.local:prometheus_cfg<0.683.0>:prometheus_cfg:apply_config:724]Restarting Prometheus as the start specs have changed
[error_logger:info,2024-10-08T19:52:23.743Z,ns_1@cb.local:ale_dynamic_sup<0.79.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.690.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.433Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.683.0>},
              {id,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.441Z,ns_1@cb.local:memcached_passwords<0.696.0>:memcached_cfg:init:60]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2024-10-08T19:52:24.448Z,ns_1@cb.local:memcached_passwords<0.696.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2024-10-08T19:52:24.501Z,ns_1@cb.local:memcached_passwords<0.696.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2024-10-08T19:52:24.504Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2024-10-08T19:52:24.511Z,ns_1@cb.local:memcached_passwords<0.696.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2024-10-08T19:52:24.511Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2024-10-08T19:52:24.511Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl] skipped. Retry in 1000 ms.
[error_logger:info,2024-10-08T19:52:24.512Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.696.0>},
              {id,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.517Z,ns_1@cb.local:memcached_permissions<0.699.0>:memcached_cfg:init:60]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2024-10-08T19:52:24.530Z,ns_1@cb.local:memcached_permissions<0.699.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2024-10-08T19:52:24.539Z,ns_1@cb.local:memcached_permissions<0.699.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2024-10-08T19:52:24.546Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2024-10-08T19:52:24.553Z,ns_1@cb.local:memcached_permissions<0.699.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2024-10-08T19:52:24.553Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[error_logger:info,2024-10-08T19:52:24.554Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.699.0>},
              {id,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.558Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.702.0>},
              {id,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.563Z,ns_1@cb.local:ns_node_disco_sup<0.703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.704.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.563Z,ns_1@cb.local:ns_node_disco<0.705.0>:ns_node_disco:init:111]Initting ns_node_disco with []
[ns_server:debug,2024-10-08T19:52:24.563Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[error_logger:info,2024-10-08T19:52:24.563Z,ns_1@cb.local:ns_node_disco_sup<0.703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.705.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.564Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636344}}]},
 {cookie,{sanitized,<<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}}]
[user:info,2024-10-08T19:52:24.564Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[ns_server:debug,2024-10-08T19:52:24.565Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2024-10-08T19:52:24.565Z,ns_1@cb.local:<0.707.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[ns_server:debug,2024-10-08T19:52:24.565Z,ns_1@cb.local:<0.709.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[error_logger:info,2024-10-08T19:52:24.572Z,ns_1@cb.local:ns_node_disco_sup<0.703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.710.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.585Z,ns_1@cb.local:<0.709.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[ns_server:debug,2024-10-08T19:52:24.585Z,ns_1@cb.local:<0.707.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[error_logger:info,2024-10-08T19:52:24.601Z,ns_1@cb.local:ns_config_rep_sup<0.711.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.712.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.601Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2024-10-08T19:52:24.602Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2024-10-08T19:52:24.609Z,ns_1@cb.local:ns_config_rep_sup<0.711.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.713.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.610Z,ns_1@cb.local:ns_node_disco_sup<0.703.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.711.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:24.610Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.703.0>},
              {id,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:24.611Z,ns_1@cb.local:tombstone_keeper<0.278.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2024-10-08T19:52:24.611Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.723.0>},
              {id,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.627Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.725.0>},
              {id,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.631Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.727.0>},
              {id,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.646Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.729.0>},
              {id,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.647Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.732.0>},
              {id,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.647Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.733.0>},
              {id,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.647Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.734.0>},
              {id,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.670Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.735.0>},
              {id,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.691Z,ns_1@cb.local:ns_heart_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.737.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.692Z,ns_1@cb.local:ns_heart_sup<0.736.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.739.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.692Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.736.0>},
              {id,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:24.723Z,ns_1@cb.local:ns_doctor_sup<0.743.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.744.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.749Z,ns_1@cb.local:ns_doctor_sup<0.743.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.745.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.750Z,ns_1@cb.local:<0.740.0>:restartable:start_child:92]Started child process <0.743.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2024-10-08T19:52:24.750Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.740.0>},
              {id,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:24.751Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.748.0>},
              {id,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.762Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.751.0>},
              {id,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.762Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.752.0>},
              {id,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.762Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.753.0>},
              {id,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.764Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.754.0>},
              {id,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.764Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.756.0>},
              {id,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.765Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.758.0>},
              {id,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.766Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.760.0>},
              {id,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.766Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.762.0>},
              {id,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.766Z,ns_1@cb.local:<0.764.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    encryption_service} with delay of 1000
[ns_server:debug,2024-10-08T19:52:24.767Z,ns_1@cb.local:encryption_service<0.766.0>:encryption_service:recover:199]Config marker /opt/couchbase/var/lib/couchbase/config/sm_load_config_marker doesn't exist
[error_logger:info,2024-10-08T19:52:24.768Z,ns_1@cb.local:<0.765.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.765.0>,suppress_max_restart_intensity}
    started: [{pid,<0.766.0>},
              {id,encryption_service},
              {mfargs,{encryption_service,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.771Z,ns_1@cb.local:<0.763.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.763.0>,suppress_max_restart_intensity}
    started: [{pid,<0.764.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      encryption_service}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,encryption_service},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => encryption_service,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [encryption_service],
                          restart => permanent,shutdown => 5000,
                          start => {encryption_service,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:24.771Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.763.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,encryption_service}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => encryption_service,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [encryption_service],
                         restart => permanent,shutdown => 5000,
                         start => {encryption_service,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:24.798Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.769.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.799Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.771.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.834Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.772.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.854Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.773.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.879Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.774.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.879Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.775.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.970Z,ns_1@cb.local:inet_gethost_native_sup<0.779.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.780.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2024-10-08T19:52:24.970Z,ns_1@cb.local:menelaus_web_sup<0.776.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.778.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:24.970Z,ns_1@cb.local:kernel_safe_sup<0.68.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.779.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:24.971Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2024-10-08T19:52:24.972Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2024-10-08T19:52:24.972Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2024-10-08T19:52:24.973Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2024-10-08T19:52:24.974Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2024-10-08T19:52:24.975Z,ns_1@cb.local:<0.782.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2024-10-08T19:52:24.975Z,ns_1@cb.local:<0.782.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.782.0>,menelaus_web}
    started: [{pid,<0.783.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:24.976Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2024-10-08T19:52:24.977Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2024-10-08T19:52:24.980Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2024-10-08T19:52:24.980Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2024-10-08T19:52:24.982Z,ns_1@cb.local:<0.782.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2024-10-08T19:52:24.983Z,ns_1@cb.local:<0.782.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2024-10-08T19:52:24.984Z,ns_1@cb.local:<0.782.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.782.0>,menelaus_web}
    started: [{pid,<0.800.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:24.985Z,ns_1@cb.local:<0.781.0>:restartable:start_child:92]Started child process <0.782.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2024-10-08T19:52:24.985Z,ns_1@cb.local:menelaus_web_sup<0.776.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.781.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2024-10-08T19:52:24.986Z,ns_1@cb.local:menelaus_sup<0.768.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "7.6.3-4200-enterprise".
[error_logger:info,2024-10-08T19:52:24.986Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.776.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:24.988Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.817.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.017Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.819.0>},
              {id,guardrail_monitor},
              {mfargs,{guardrail_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.030Z,ns_1@cb.local:menelaus_sup<0.768.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.820.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.031Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.768.0>},
              {id,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:25.031Z,ns_1@cb.local:<0.827.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_ports_setup} with delay of 4000
[error_logger:info,2024-10-08T19:52:25.032Z,ns_1@cb.local:<0.828.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.828.0>,suppress_max_restart_intensity}
    started: [{pid,<0.829.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.033Z,ns_1@cb.local:<0.826.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.826.0>,suppress_max_restart_intensity}
    started: [{pid,<0.827.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_ports_setup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_ports_setup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_ports_setup,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => brutal_kill,
                          start => {ns_ports_setup,start,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.033Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.826.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_ports_setup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_ports_setup,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => brutal_kill,
                         start => {ns_ports_setup,start,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:25.045Z,ns_1@cb.local:ns_ports_setup<0.829.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2024-10-08T19:52:25.050Z,ns_1@cb.local:service_agent_sup<0.832.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.833.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.051Z,ns_1@cb.local:service_agent_sup<0.832.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.834.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.125430937>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.051Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.832.0>},
              {id,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:error,2024-10-08T19:52:25.079Z,ns_1@cb.local:<0.777.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bname%3D~%60kv_curr_items%7Ckv_curr_items_tot%7Ckv_mem_used_bytes%7Ccouch_docs_actual_disk_size%7Ccouch_views_actual_disk_size%7Ckv_ep_db_data_size_bytes%7Ckv_ep_bg_fetched%60%7D+or+kv_vb_curr_items%7Bstate%3D%27replica%27%7D+or+kv_vb_num_non_resident%7Bstate%3D%27active%27%7D+or+label_replace%28sum+by+%28bucket%2C+name%29+%28irate%28kv_ops%7Bop%3D%60get%60%7D%5B1m%5D%29%29%2C+%60name%60%2C%60cmd_get%60%2C+%60%60%2C+%60%60%29+or+label_replace%28irate%28kv_ops%7Bop%3D%60get%60%2Cresult%3D%60hit%60%7D%5B1m%5D%29%2C%60name%60%2C%60get_hits%60%2C%60%60%2C%60%60%29+or+label_replace%28sum+by+%28bucket%29+%28irate%28kv_cmd_lookup%5B1m%5D%29+or+irate%28kv_ops%7Bop%3D~%60set%7Cincr%7Cdecr%7Cdelete%7Cdel_meta%7Cget_meta%7Cset_meta%7Cset_ret_meta%7Cdel_ret_meta%60%7D%5B1m%5D%29%29%2C+%60name%60%2C+%60ops%60%2C+%60%60%2C+%60%60%29+or+sum+by+%28bucket%2C+name%29+%28%7Bname%3D~%60index_data_size%7Cindex_disk_size%7Ccouch_spatial_data_size%7Ccouch_spatial_disk_size%7Ccouch_views_data_size%60%7D%29&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:error,2024-10-08T19:52:25.089Z,ns_1@cb.local:<0.839.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system-processes%60%2Cname%3D~%60sysproc_mem_resident%7Csysproc_mem_size%7Csysproc_cpu_utilization%7Csysproc_major_faults_raw%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:error,2024-10-08T19:52:25.091Z,ns_1@cb.local:<0.842.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system%60%2Cname%3D~%60sys_cpu_utilization_rate%7Csys_cpu_stolen_rate%7Csys_swap_total%7Csys_swap_used%7Csys_mem_total%7Csys_mem_free%7Csys_mem_limit%7Csys_cpu_cores_available%7Csys_allocstall%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[error_logger:info,2024-10-08T19:52:25.107Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.843.0>},
              {id,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.140Z,ns_1@cb.local:memcached_auth_server<0.876.0>:memcached_auth_server:reconnect:239]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2024-10-08T19:52:25.142Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.876.0>},
              {id,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.151Z,ns_1@cb.local:<0.879.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit_cfg} with delay of 4000
[ns_server:debug,2024-10-08T19:52:25.153Z,ns_1@cb.local:ns_audit_cfg<0.881.0>:ns_audit_cfg:write_audit_json:238]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "33985209"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {prune_age,
                                                                                      0},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2024-10-08T19:52:25.200Z,ns_1@cb.local:ns_heart<0.737.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2024-10-08T19:52:25.212Z,ns_1@cb.local:ns_heart<0.737.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:40]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[ns_server:debug,2024-10-08T19:52:25.216Z,ns_1@cb.local:ns_audit_cfg<0.881.0>:ns_audit_cfg:notify_memcached:152]Instruct memcached to reload audit config
[error_logger:info,2024-10-08T19:52:25.216Z,ns_1@cb.local:<0.880.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.880.0>,suppress_max_restart_intensity}
    started: [{pid,<0.881.0>},
              {id,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.217Z,ns_1@cb.local:<0.878.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.878.0>,suppress_max_restart_intensity}
    started: [{pid,<0.879.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit_cfg}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit_cfg},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit_cfg,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.217Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.878.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit_cfg}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit_cfg,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:25.217Z,ns_1@cb.local:<0.887.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit} with delay of 4000
[ns_server:warn,2024-10-08T19:52:25.224Z,ns_1@cb.local:<0.885.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2024-10-08T19:52:25.242Z,ns_1@cb.local:<0.888.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.888.0>,suppress_max_restart_intensity}
    started: [{pid,<0.889.0>},
              {id,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.243Z,ns_1@cb.local:<0.886.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.886.0>,suppress_max_restart_intensity}
    started: [{pid,<0.887.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.244Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.886.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:25.244Z,ns_1@cb.local:<0.892.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    memcached_config_mgr} with delay of 4000
[ns_server:debug,2024-10-08T19:52:25.244Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2024-10-08T19:52:25.244Z,ns_1@cb.local:<0.893.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.893.0>,suppress_max_restart_intensity}
    started: [{pid,<0.894.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.245Z,ns_1@cb.local:<0.891.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.891.0>,suppress_max_restart_intensity}
    started: [{pid,<0.892.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      memcached_config_mgr}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,memcached_config_mgr},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => memcached_config_mgr,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start => {memcached_config_mgr,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.246Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.891.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,memcached_config_mgr}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => memcached_config_mgr,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start => {memcached_config_mgr,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2024-10-08T19:52:25.255Z,ns_1@cb.local:<0.896.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2024-10-08T19:52:25.255Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.896.0>},
              {id,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.261Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.899.0>},
              {id,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.275Z,ns_1@cb.local:<0.903.0>:memcached_config_mgr:memcached_port_pid:154]waiting for completion of initial ns_ports_setup round
[error_logger:info,2024-10-08T19:52:25.275Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.903.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:error,2024-10-08T19:52:25.303Z,ns_1@cb.local:<0.913.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bname%3D~%60kv_curr_items%7Ckv_curr_items_tot%7Ckv_mem_used_bytes%7Ccouch_docs_actual_disk_size%7Ccouch_views_actual_disk_size%7Ckv_ep_db_data_size_bytes%7Ckv_ep_bg_fetched%60%7D+or+kv_vb_curr_items%7Bstate%3D%27replica%27%7D+or+kv_vb_num_non_resident%7Bstate%3D%27active%27%7D+or+label_replace%28sum+by+%28bucket%2C+name%29+%28irate%28kv_ops%7Bop%3D%60get%60%7D%5B1m%5D%29%29%2C+%60name%60%2C%60cmd_get%60%2C+%60%60%2C+%60%60%29+or+label_replace%28irate%28kv_ops%7Bop%3D%60get%60%2Cresult%3D%60hit%60%7D%5B1m%5D%29%2C%60name%60%2C%60get_hits%60%2C%60%60%2C%60%60%29+or+label_replace%28sum+by+%28bucket%29+%28irate%28kv_cmd_lookup%5B1m%5D%29+or+irate%28kv_ops%7Bop%3D~%60set%7Cincr%7Cdecr%7Cdelete%7Cdel_meta%7Cget_meta%7Cset_meta%7Cset_ret_meta%7Cdel_ret_meta%60%7D%5B1m%5D%29%29%2C+%60name%60%2C+%60ops%60%2C+%60%60%2C+%60%60%29+or+sum+by+%28bucket%2C+name%29+%28%7Bname%3D~%60index_data_size%7Cindex_disk_size%7Ccouch_spatial_data_size%7Ccouch_spatial_disk_size%7Ccouch_views_data_size%60%7D%29&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:error,2024-10-08T19:52:25.305Z,ns_1@cb.local:<0.918.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system-processes%60%2Cname%3D~%60sysproc_mem_resident%7Csysproc_mem_size%7Csysproc_cpu_utilization%7Csysproc_major_faults_raw%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:error,2024-10-08T19:52:25.307Z,ns_1@cb.local:<0.921.0>:prometheus:post_async:217]Prometheus http request failed:
URL: http://127.0.0.1:9123/api/v1/query
Body: query=%7Bcategory%3D%60system%60%2Cname%3D~%60sys_cpu_utilization_rate%7Csys_cpu_stolen_rate%7Csys_swap_total%7Csys_swap_used%7Csys_mem_total%7Csys_mem_free%7Csys_mem_limit%7Csys_cpu_cores_available%7Csys_allocstall%60%7D&timeout=5s
Reason: {failed_connect,[{to_address,{"127.0.0.1",9123}},
                         {inet,[inet],econnrefused}]}
[ns_server:debug,2024-10-08T19:52:25.313Z,ns_1@cb.local:ns_heart_slow_status_updater<0.739.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2024-10-08T19:52:25.314Z,ns_1@cb.local:ns_heart_slow_status_updater<0.739.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:40]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2024-10-08T19:52:25.319Z,ns_1@cb.local:ns_bucket_worker_sup<0.905.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.923.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.339Z,ns_1@cb.local:ns_bucket_worker_sup<0.905.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.926.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.340Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.905.0>},
              {id,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.340Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.930.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2024-10-08T19:52:25.342Z,ns_1@cb.local:<0.929.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2024-10-08T19:52:25.379Z,ns_1@cb.local:<0.932.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.930.0>} exited with reason {noproc,
                                                                                {gen_statem,
                                                                                 call,
                                                                                 [mb_master,
                                                                                  master_node,
                                                                                  infinity]}}
[error_logger:info,2024-10-08T19:52:25.445Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.941.0>},
              {id,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.446Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.943.0>},
              {id,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.446Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.945.0>},
              {id,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.447Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.947.0>},
              {id,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.472Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.949.0>},
              {id,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.475Z,ns_1@cb.local:goxdcr_status_keeper<0.949.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2024-10-08T19:52:25.480Z,ns_1@cb.local:goxdcr_status_keeper<0.949.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2024-10-08T19:52:25.509Z,ns_1@cb.local:services_stats_sup<0.958.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.959.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:warn,2024-10-08T19:52:25.513Z,ns_1@cb.local:<0.960.0>:ns_memcached:connect:1457]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2024-10-08T19:52:25.513Z,ns_1@cb.local:<0.960.0>:memcached_refresh:do_refresh:171]Failed to connect to memcached: couldnt_connect_to_memcached
[ns_server:debug,2024-10-08T19:52:25.513Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:update_refresh_state:137]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2024-10-08T19:52:25.536Z,ns_1@cb.local:service_status_keeper_sup<0.967.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.968.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.595Z,ns_1@cb.local:service_status_keeper_sup<0.967.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.972.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.606Z,ns_1@cb.local:service_status_keeper_sup<0.967.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.975.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.612Z,ns_1@cb.local:service_status_keeper_sup<0.967.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.978.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.612Z,ns_1@cb.local:services_stats_sup<0.958.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.967.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.618Z,ns_1@cb.local:services_stats_sup<0.958.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.981.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,[#Fun<services_stats_sup.0.35188242>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.618Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.958.0>},
              {id,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:25.619Z,ns_1@cb.local:<0.984.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    compaction_daemon} with delay of 4000
[ns_server:debug,2024-10-08T19:52:25.649Z,ns_1@cb.local:<0.989.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.989.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2024-10-08T19:52:25.654Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[error_logger:info,2024-10-08T19:52:25.655Z,ns_1@cb.local:<0.985.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.985.0>,suppress_max_restart_intensity}
    started: [{pid,<0.986.0>},
              {id,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.655Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:52:25.655Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:52:25.655Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[error_logger:info,2024-10-08T19:52:25.655Z,ns_1@cb.local:<0.983.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.983.0>,suppress_max_restart_intensity}
    started: [{pid,<0.984.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      compaction_daemon}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,compaction_daemon},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => compaction_daemon,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [compaction_daemon],
                          restart => permanent,shutdown => 86400000,
                          start => {compaction_daemon,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:25.655Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:52:25.655Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2024-10-08T19:52:25.655Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.983.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,compaction_daemon}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => compaction_daemon,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [compaction_daemon],
                         restart => permanent,shutdown => 86400000,
                         start => {compaction_daemon,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.660Z,ns_1@cb.local:cluster_logs_sup<0.990.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.991.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.661Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.990.0>},
              {id,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.662Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.992.0>},
              {id,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.663Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.994.0>},
              {id,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.683Z,ns_1@cb.local:leader_leases_sup<0.997.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.998.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.695Z,ns_1@cb.local:leader_leases_sup<0.997.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.999.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.696Z,ns_1@cb.local:leader_services_sup<0.996.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.997.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:25.708Z,ns_1@cb.local:leader_registry_sup<0.1000.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.1001.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.708Z,ns_1@cb.local:leader_registry_sup<0.1000.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2024-10-08T19:52:25.709Z,ns_1@cb.local:leader_registry_sup<0.1000.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2024-10-08T19:52:25.709Z,ns_1@cb.local:leader_registry_sup<0.1000.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2024-10-08T19:52:25.710Z,ns_1@cb.local:mb_master<0.1003.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2024-10-08T19:52:25.710Z,ns_1@cb.local:mb_master<0.1003.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:debug,2024-10-08T19:52:25.710Z,ns_1@cb.local:leader_registry<0.1001.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2024-10-08T19:52:25.743Z,ns_1@cb.local:mb_master<0.1003.0>:master_activity_events:submit_cast:75]Failed to send master activity event {became_master,'ns_1@cb.local'}: {error,
                                                                       badarg}
[error_logger:info,2024-10-08T19:52:25.752Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1006.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.761Z,ns_1@cb.local:leader_quorum_nodes_manager<0.1008.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2024-10-08T19:52:25.762Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1008.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.762Z,ns_1@cb.local:leader_quorum_nodes_manager<0.1008.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:info,2024-10-08T19:52:25.775Z,ns_1@cb.local:mb_master_sup<0.1005.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.1013.0> on 'ns_1@cb.local'

[error_logger:info,2024-10-08T19:52:25.775Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1013.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:25.779Z,ns_1@cb.local:leader_lease_agent<0.999.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"2af0d1ad1febc1e3df15b93b601fdf11">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:debug,2024-10-08T19:52:25.786Z,ns_1@cb.local:<0.1016.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.1492486459.688390145.212585>
[ns_server:info,2024-10-08T19:52:25.787Z,ns_1@cb.local:mb_master_sup<0.1005.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.1016.0> on 'ns_1@cb.local'

[error_logger:info,2024-10-08T19:52:25.787Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1016.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:25.805Z,ns_1@cb.local:ns_orchestrator_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.1019.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:25.821Z,ns_1@cb.local:compat_mode_manager<0.1020.0>:cluster_compat_mode:do_upgrades:205]Initiating rbac upgrade due to version change from [7,1] to [7,6] (target version: [7,
                                                                                    6])
[ns_server:info,2024-10-08T19:52:25.822Z,ns_1@cb.local:compat_mode_manager<0.1020.0>:menelaus_users:upgrade:1057]Upgrading users database to [7,6]
[ns_server:info,2024-10-08T19:52:25.824Z,ns_1@cb.local:<0.1012.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"2af0d1ad1febc1e3df15b93b601fdf11">>)
[ns_server:debug,2024-10-08T19:52:25.824Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade]..)
[ns_server:debug,2024-10-08T19:52:25.824Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636345}}]}|
 started]
[ns_server:debug,2024-10-08T19:52:25.839Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 11 us
[ns_server:debug,2024-10-08T19:52:25.840Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2024-10-08T19:52:25.840Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 16 us
[ns_server:debug,2024-10-08T19:52:25.840Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2024-10-08T19:52:25.840Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2024-10-08T19:52:25.840Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:handle_call:146]Received sync_token from {<0.1036.0>,
                          [alias|#Ref<0.1492486459.688455682.213247>]}
[ns_server:debug,2024-10-08T19:52:25.841Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:95]Received sync_token from {<0.1036.0>,
                          [alias|#Ref<0.1492486459.688455682.213247>]}
[ns_server:debug,2024-10-08T19:52:25.841Z,ns_1@cb.local:<0.1033.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:debug,2024-10-08T19:52:25.841Z,ns_1@cb.local:users_storage<0.436.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,'_','_','_'},[],['$_']}],
                               100}
[ns_server:info,2024-10-08T19:52:25.841Z,ns_1@cb.local:compat_mode_manager<0.1020.0>:menelaus_users:upgrade:1071]Users database was upgraded to [7,6]
[ns_server:debug,2024-10-08T19:52:25.841Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2024-10-08T19:52:25.841Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2024-10-08T19:52:25.842Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:handle_call:146]Received sync_token from {<0.1040.0>,
                          [alias|#Ref<0.1492486459.688455682.213266>]}
[ns_server:debug,2024-10-08T19:52:25.842Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:95]Received sync_token from {<0.1040.0>,
                          [alias|#Ref<0.1492486459.688455682.213266>]}
[ns_server:debug,2024-10-08T19:52:25.842Z,ns_1@cb.local:<0.1037.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:info,2024-10-08T19:52:25.843Z,ns_1@cb.local:compat_mode_manager<0.1020.0>:menelaus_users:upgrade:1073]Users database upgrade was delivered to ['ns_1@cb.local']
[ns_server:info,2024-10-08T19:52:25.843Z,ns_1@cb.local:kv<0.269.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,1]. Final version = [7,6]
[ns_server:info,2024-10-08T19:52:25.843Z,ns_1@cb.local:kv<0.269.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,2]. Final version = [7,6]
[ns_server:debug,2024-10-08T19:52:25.883Z,ns_1@cb.local:chronicle_kv_log<0.666.0>:chronicle_kv_log:log:59]update (key: cluster_compat_version, rev: {<<"05f558c2457ead34373f1c979f713946">>,
                                           6})
[7,6]
[ns_server:debug,2024-10-08T19:52:25.886Z,ns_1@cb.local:roles_cache<0.451.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2024-10-08T19:52:25.886Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2024-10-08T19:52:25.886Z,ns_1@cb.local:memcached_permissions<0.699.0>:memcached_permissions:producer:512]Skipping update during users upgrade
[ns_server:debug,2024-10-08T19:52:25.886Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2024-10-08T19:52:25.886Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              6],
                                                                             {0,
                                                                              464747793},
                                                                             {0,
                                                                              464747793},
                                                                             false,
                                                                             []}
[ns_server:debug,2024-10-08T19:52:25.886Z,ns_1@cb.local:tombstone_keeper<0.278.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2024-10-08T19:52:25.886Z,ns_1@cb.local:<0.1043.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[ns_server:debug,2024-10-08T19:52:25.887Z,ns_1@cb.local:<0.1043.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[ns_server:debug,2024-10-08T19:52:25.888Z,ns_1@cb.local:memcached_passwords<0.696.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2024-10-08T19:52:25.888Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,1]}]

[ns_server:info,2024-10-08T19:52:25.890Z,ns_1@cb.local:ns_config<0.281.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,2]
[ns_server:info,2024-10-08T19:52:25.892Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,2]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]}]

[ns_server:info,2024-10-08T19:52:25.950Z,ns_1@cb.local:ns_config<0.281.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,6]
[ns_server:debug,2024-10-08T19:52:25.970Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1146]Going to notify following services: [memcached]
[ns_server:info,2024-10-08T19:52:25.971Z,ns_1@cb.local:<0.1049.0>:ns_ssl_services_setup:notify_service:1182]Successfully notified service memcached
[ns_server:info,2024-10-08T19:52:25.971Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1162]Succesfully notified services [memcached]
[ns_server:debug,2024-10-08T19:52:26.083Z,ns_1@cb.local:memcached_passwords<0.696.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2024-10-08T19:52:26.101Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2024-10-08T19:52:26.102Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588778000
[ns_server:debug,2024-10-08T19:52:26.112Z,ns_1@cb.local:memcached_passwords<0.696.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2024-10-08T19:52:26.112Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:info,2024-10-08T19:52:26.105Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,6]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {8265,
        [{name,<<"RBAC information retrieved">>},
         {description,<<"RBAC information was retrieved">>},
         {enabled,true},
         {module,ns_server}]},
       {16384,
        [{name,<<"remote cluster ref creation">>},
         {description,<<"created remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16385,
        [{name,<<"remote cluster ref update">>},
         {description,<<"updated remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16386,
        [{name,<<"remote cluster ref deletion">>},
         {description,<<"deleted remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16387,
        [{name,<<"replication creation">>},
         {description,<<"created replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16388,
        [{name,<<"replication pause">>},
         {description,<<"paused replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16389,
        [{name,<<"replication resume">>},
         {description,<<"resumed replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16390,
        [{name,<<"replication cancellation">>},
         {description,<<"canceled replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16391,
        [{name,<<"default replication settings update">>},
         {description,<<"updated default replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16392,
        [{name,<<"individual replication settings update">>},
         {description,<<"updated individual replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16393,
        [{name,<<"bucket settings update">>},
         {description,<<"updated bucket settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16394,
        [{name,<<"authorization failure while adding remote cluster ref">>},
         {description,<<"failed to add remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16395,
        [{name,<<"authorization failure while updating remote cluster ref">>},
         {description,<<"failed to update remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16396,
        [{name,<<"access denied">>},
         {description,<<"access denied">>},
         {enabled,true},
         {module,xdcr}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {20493,
        [{name,<<"session terminated">>},
         {description,<<"Session to the cluster has terminated">>},
         {enabled,false},
         {module,memcached}]},
       {20494,
        [{name,<<"tenant rate limited">>},
         {description,<<"The given tenant was rate limited">>},
         {enabled,true},
         {module,memcached}]},
       {24576,
        [{name,<<"Delete index">>},
         {description,<<"FTS index was deleted">>},
         {enabled,true},
         {module,fts}]},
       {24577,
        [{name,<<"Create/Update index">>},
         {description,<<"FTS index was created/Updated">>},
         {enabled,true},
         {module,fts}]},
       {24579,
        [{name,<<"Control index">>},
         {description,<<"FTS index control command was issued">>},
         {enabled,true},
         {module,fts}]},
       {24582,
        [{name,<<"GC run">>},
         {description,<<"GC run was triggered">>},
         {enabled,true},
         {module,fts}]},
       {24583,
        [{name,<<"CPU profile">>},
         {description,<<"CPU profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {24584,
        [{name,<<"Memory profile">>},
         {description,<<"Memory profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28706,
        [{name,<<"CREATE FUNCTION statement">>},
         {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28707,
        [{name,<<"DROP FUNCTION statement">>},
         {description,<<"A N1QL DROP FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28708,
        [{name,<<"EXECUTE FUNCTION statement">>},
         {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28709,
        [{name,<<"/admin/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28710,
        [{name,<<"/admin/indexes/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28711,
        [{name,<<"/admin/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28712,
        [{name,<<"/admin/indexes/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28713,
        [{name,<<"CREATE SCOPE statement">>},
         {description,<<"A N1QL CREATE SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28714,
        [{name,<<"DROP SCOPE statement">>},
         {description,<<"A N1QL DROP SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28715,
        [{name,<<"CREATE COLLECTION statement">>},
         {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28716,
        [{name,<<"DROP COLLECTION statement">>},
         {description,<<"A N1QL DROP COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28717,
        [{name,<<"FLUSH COLLECTION statement">>},
         {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28718,
        [{name,<<"UPDATE STATISTICS statement">>},
         {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28719,
        [{name,<<"ADVISE statement">>},
         {description,<<"A N1QL ADVISE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28720,
        [{name,<<"START TRANSACTION statement">>},
         {description,<<"A N1QL START TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28721,
        [{name,<<"COMMIT TRANSACTION statement">>},
         {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28722,
        [{name,<<"ROLLBACK TRANSACTION statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28723,
        [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28724,
        [{name,<<"SET TRANSACTION ISOLATION statement">>},
         {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28725,
        [{name,<<"SAVEPOINT statement">>},
         {description,<<"A N1QL SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28726,
        [{name,<<"/admin/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28727,
        [{name,<<"/admin/indexes/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28728,
        [{name,<<"N1QL backup / restore API request">>},
         {description,<<"An HTTP request was made to archive or restore N1QL metadata">>},
         {enabled,false},
         {module,n1ql}]},
       {28729,
        [{name,<<"/admin/shutdown API request">>},
         {description,<<"An HTTP request was made to initiate graceful shutdown">>},
         {enabled,false},
         {module,n1ql}]},
       {28730,
        [{name,<<"/admin/gc API request">>},
         {description,<<"An HTTP request was made to run garbage collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28731,
        [{name,<<"/admin/ffdc API request">>},
         {description,<<"An HTTP request was made to run an FFDC collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28732,
        [{name,<<"/admin/log/ API request">>},
         {description,<<"An HTTP request was made to access diagnostic logs">>},
         {enabled,false},
         {module,n1ql}]},
       {28733,
        [{name,<<"/admin/sequences_cache API request">>},
         {description,<<"An HTTP request was made to access sequences">>},
         {enabled,false},
         {module,n1ql}]},
       {28734,
        [{name,<<"CREATE SEQUENCE statement">>},
         {description,<<"A N1QL CREATE SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28735,
        [{name,<<"ALTER SEQUENCE statement">>},
         {description,<<"A N1QL ALTER SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28736,
        [{name,<<"DROP SEQUENCE statement">>},
         {description,<<"A N1QL DROP SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28737,
        [{name,<<"Migration abort">>},
         {description,<<"Migration was aborted">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Request to create or update eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Request to delete eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Request to fetch eventing function definition">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Request to fetch eventing deployed functions list">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Request to fetch eventing function draft definitions">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Request to delete eventing function draft definitions">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Request to save a draft definition">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Request to start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Request to stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Request to start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Request to stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Request to save settings for an eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Request to fetch eventing config">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Request to save eventing config">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Request to fetch eventing function settings">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Request to import one or more eventing functions">>},
         {enabled,true},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Request to export all eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Request to fetch eventing running function list">>},
         {enabled,false},
         {module,eventing}]},
       {32789,
        [{name,<<"Deploy Function">>},
         {description,<<"Request to deploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32790,
        [{name,<<"Undeploy Function">>},
         {description,<<"Request to undeploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32791,
        [{name,<<"Pause Function">>},
         {description,<<"Request to pause eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32792,
        [{name,<<"Resume Function">>},
         {description,<<"Request to resume eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32793,
        [{name,<<"Backup Functions">>},
         {description,<<"Request to backup one or more eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32794,
        [{name,<<"Restore Functions">>},
         {description,<<"Request to restore one or more eventing functions from a backup">>},
         {enabled,true},
         {module,eventing}]},
       {32795,
        [{name,<<"List Function">>},
         {description,<<"Request to fetch eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32796,
        [{name,<<"Function Status">>},
         {description,<<"Request to fetch eventing function status">>},
         {enabled,false},
         {module,eventing}]},
       {32797,
        [{name,<<"Clear Stats">>},
         {description,<<"Request to reset eventing function stats">>},
         {enabled,true},
         {module,eventing}]},
       {32798,
        [{name,<<"Fetch Stats">>},
         {description,<<"Request to fetch eventing function stats">>},
         {enabled,false},
         {module,eventing}]},
       {32799,
        [{name,<<"Eventing Cluster Stats">>},
         {description,<<"Request to fetch eventing cluster stats">>},
         {enabled,false},
         {module,eventing}]},
       {32801,
        [{name,<<"Eventing System Event">>},
         {description,<<"Request to execute eventing node related functions">>},
         {enabled,false},
         {module,eventing}]},
       {32802,
        [{name,<<"Get User Info">>},
         {description,<<"Request to get real_userid eventing permissions">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36867,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36868,
        [{name,<<"CREATE DATAVERSE statement">>},
         {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36869,
        [{name,<<"DROP DATAVERSE statement">>},
         {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36870,
        [{name,<<"CREATE DATASET statement">>},
         {description,<<"A N1QL CREATE DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36871,
        [{name,<<"DROP DATASET statement">>},
         {description,<<"A N1QL DROP DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36872,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36873,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36877,
        [{name,<<"CONNECT LINK statement">>},
         {description,<<"A N1QL CONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36878,
        [{name,<<"DISCONNECT LINK statement">>},
         {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36879,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
         {enabled,false},
         {module,analytics}]},
       {36880,
        [{name,<<"ALTER COLLECTION statement">>},
         {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]},
       {40966,
        [{name,<<"Access denied">>},
         {description,<<"Access denied to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,view_engine}]},
       {45056,
        [{name,<<"Modify configuration">>},
         {description,<<"Backup service configuration was modified">>},
         {enabled,true},
         {module,backup}]},
       {45057,
        [{name,<<"Fetch configuration">>},
         {description,<<"Backup service configuration was retrieved">>},
         {enabled,false},
         {module,backup}]},
       {45058,
        [{name,<<"Add plan">>},
         {description,<<"A new backup plan was added">>},
         {enabled,true},
         {module,backup}]},
       {45059,
        [{name,<<"Modify plan">>},
         {description,<<"Existing backup plan was modified">>},
         {enabled,true},
         {module,backup}]},
       {45060,
        [{name,<<"Delete plan">>},
         {description,<<"A backup plan was removed">>},
         {enabled,true},
         {module,backup}]},
       {45061,
        [{name,<<"Fetch plan">>},
         {description,<<"One or more backup plans where fetched">>},
         {enabled,false},
         {module,backup}]},
       {45062,
        [{name,<<"Add repository">>},
         {description,<<"A new active backup repository was added">>},
         {enabled,true},
         {module,backup}]},
       {45063,
        [{name,<<"Archive repository">>},
         {description,<<"An active repository was archived">>},
         {enabled,true},
         {module,backup}]},
       {45064,
        [{name,<<"Pause repository">>},
         {description,<<"An active repository was paused">>},
         {enabled,true},
         {module,backup}]},
       {45065,
        [{name,<<"Resume repository">>},
         {description,<<"An active repository was resumed">>},
         {enabled,true},
         {module,backup}]},
       {45066,
        [{name,<<"Fetch repository">>},
         {description,<<"A repository was fetched">>},
         {enabled,false},
         {module,backup}]},
       {45067,
        [{name,<<"Restore repository">>},
         {description,<<"The repository data was restored">>},
         {enabled,true},
         {module,backup}]},
       {45068,
        [{name,<<"Backup repository">>},
         {description,<<"A manual backup was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45069,
        [{name,<<"Merge repository">>},
         {description,<<"A manual merge was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45070,
        [{name,<<"Info repository">>},
         {description,<<"Information about the structure and contents of the backup repository was fetched.">>},
         {enabled,false},
         {module,backup}]},
       {45071,
        [{name,<<"Examine repository">>},
         {description,<<"A document was retrieved from the repository backups">>},
         {enabled,true},
         {module,backup}]},
       {45072,
        [{name,<<"Delete repository">>},
         {description,<<"A repository was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45073,
        [{name,<<"Delete backup">>},
         {description,<<"An active repository backup was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45074,
        [{name,<<"Access denied">>},
         {description,<<"A user has been denied access to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,backup}]}]},
 {delete,rbac_upgrade},
 {set,auto_failover_cfg,
      [{disable_max_count,false},
       {enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]},
 {delete,memory_alert_email},
 {delete,memory_alert_popup},
 {delete,popup_alerts_auto_failover_upgrade_70_fixed},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_shard_affinity\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.blob_storage_bucket\":\"\",\"indexer.settings.rebalance.blob_storage_prefix\":\"\",\"indexer.settings.rebalance.blob_storage_region\":\"\",\"indexer.settings.rebalance.blob_storage_scheme\":\"\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.thresholds.mem_high\":70,\"indexer.settings.thresholds.mem_low\":50,\"indexer.settings.thresholds.units_high\":60,\"indexer.settings.thresholds.units_low\":40}">>},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true,\"use-replica\":\"unset\"}">>},
 {set,{metakv,<<"/analytics/settings/config">>},
      <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>},
 {delete,mb33750_workaround_enabled},
 {delete,cert_and_pkey},
 {set,resource_management,
      [{bucket,[{resident_ratio,[{enabled,false},
                                 {couchstore_minimum,1},
                                 {magma_minimum,0.2}]},
                {data_size,[{enabled,false},
                            {couchstore_maximum,2},
                            {magma_maximum,16}]}]},
       {index,[]},
       {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
       {disk_usage,[{enabled,false},{maximum,96}]},
       {collections_per_quota,[{enabled,false},{maximum,1}]}]}]

[ns_server:debug,2024-10-08T19:52:26.301Z,ns_1@cb.local:ns_ports_setup<0.829.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <17008.134.0>
[ns_server:debug,2024-10-08T19:52:26.304Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2024-10-08T19:52:26.304Z,ns_1@cb.local:<0.903.0>:memcached_config_mgr:memcached_port_pid:156]ns_ports_setup seems to be ready
[ns_server:debug,2024-10-08T19:52:26.305Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <17008.140.0>
[ns_server:debug,2024-10-08T19:52:26.306Z,ns_1@cb.local:<0.903.0>:memcached_config_mgr:find_port_pid_loop:164]Found memcached port <17008.140.0>
[ns_server:debug,2024-10-08T19:52:26.326Z,ns_1@cb.local:roles_cache<0.451.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2024-10-08T19:52:26.326Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2024-10-08T19:52:26.345Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:832]Considering to store CA certs
[ns_server:debug,2024-10-08T19:52:26.345Z,ns_1@cb.local:<0.1056.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[ns_server:debug,2024-10-08T19:52:26.350Z,ns_1@cb.local:memcached_permissions<0.699.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2024-10-08T19:52:26.350Z,ns_1@cb.local:tombstone_keeper<0.278.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2024-10-08T19:52:26.350Z,ns_1@cb.local:prometheus_cfg<0.683.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2024-10-08T19:52:26.351Z,ns_1@cb.local:<0.1056.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"gKubpTtk1rsPfh+NITxcXJBIdxAm9WKp2TSTrh0UWqU=">>}
[ns_server:debug,2024-10-08T19:52:26.350Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
audit_decriptors ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636346}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {16384,
  [{name,<<"remote cluster ref creation">>},
   {description,<<"created remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16385,
  [{name,<<"remote cluster ref update">>},
   {description,<<"updated remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16386,
  [{name,<<"remote cluster ref deletion">>},
   {description,<<"deleted remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16387,
  [{name,<<"replication creation">>},
   {description,<<"created replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16388,
  [{name,<<"replication pause">>},
   {description,<<"paused replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16389,
  [{name,<<"replication resume">>},
   {description,<<"resumed replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16390,
  [{name,<<"replication cancellation">>},
   {description,<<"canceled replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16391,
  [{name,<<"default replication settings update">>},
   {description,<<"updated default replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16392,
  [{name,<<"individual replication settings update">>},
   {description,<<"updated individual replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16393,
  [{name,<<"bucket settings update">>},
   {description,<<"updated bucket settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16394,
  [{name,<<"authorization failure while adding remote cluster ref">>},
   {description,<<"failed to add remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16395,
  [{name,<<"authorization failure while updating remote cluster ref">>},
   {description,<<"failed to update remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16396,
  [{name,<<"access denied">>},
   {description,<<"access denied">>},
   {enabled,true},
   {module,xdcr}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {24576,
  [{name,<<"Delete index">>},
   {description,<<"FTS index was deleted">>},
   {enabled,true},
   {module,fts}]},
 {24577,
  [{name,<<"Create/Update index">>},
   {description,<<"FTS index was created/Updated">>},
   {enabled,true},
   {module,fts}]},
 {24579,
  [{name,<<"Control index">>},
   {description,<<"FTS index control command was issued">>},
   {enabled,true},
   {module,fts}]},
 {24582,
  [{name,<<"GC run">>},
   {description,<<"GC run was triggered">>},
   {enabled,true},
   {module,fts}]},
 {24583,
  [{name,<<"CPU profile">>},
   {description,<<"CPU profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {24584,
  [{name,<<"Memory profile">>},
   {description,<<"Memory profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was execu"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statemen"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT st"...>>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO S"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statem"...>>},
   {description,<<"A N1QL SET TRANSACTION ISOLA"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT stateme"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API "...>>},
   {description,<<"An HTTP request was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/trans"...>>},
   {description,<<"An HTTP request "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / re"...>>},
   {description,<<"An HTTP requ"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutd"...>>},
   {description,<<"An HTTP "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28730,
  [{name,<<"/admin/g"...>>},
   {description,<<"An H"...>>},
   {enabled,false},
   {module,...}]},
 {28731,[{name,<<"/adm"...>>},{description,<<...>>},{enabled,...},{...}]},
 {28732,[{name,<<...>>},{description,...},{...}|...]},
 {28733,[{name,...},{...}|...]},
 {28734,[{...}|...]},
 {28735,[...]},
 {28736,...},
 {...}|...]
[ns_server:debug,2024-10-08T19:52:26.369Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([audit_decriptors,auto_failover_cfg,
                               cluster_compat_version,rbac_upgrade,
                               resource_management,
                               {metakv,<<"/analytics/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {node,'ns_1@cb.local',prometheus_auth_info}]..)
[ns_server:debug,2024-10-08T19:52:26.373Z,ns_1@cb.local:memcached_permissions<0.699.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2024-10-08T19:52:26.373Z,ns_1@cb.local:memcached_passwords<0.696.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:warn,2024-10-08T19:52:26.374Z,ns_1@cb.local:<0.885.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2024-10-08T19:52:26.378Z,ns_1@cb.local:prometheus_cfg<0.683.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:warn,2024-10-08T19:52:26.380Z,ns_1@cb.local:<0.929.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2024-10-08T19:52:26.383Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
cluster_compat_version ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{3,63895636346}}]},7,6]
[ns_server:debug,2024-10-08T19:52:26.383Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{2,63895636346}}]}|
 '_deleted']
[ns_server:debug,2024-10-08T19:52:26.383Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{2,63895636346}}]},
 {disable_max_count,false},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2024-10-08T19:52:26.384Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 27 us
[ns_server:debug,2024-10-08T19:52:26.385Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2024-10-08T19:52:26.385Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 5 us
[user:warn,2024-10-08T19:52:26.385Z,ns_1@cb.local:compat_mode_manager<0.1020.0>:compat_mode_manager:handle_consider_switching_compat_mode:43]Changed cluster compat mode from [7,1] to [7,6]
[error_logger:info,2024-10-08T19:52:26.386Z,ns_1@cb.local:ns_orchestrator_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.1020.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:26.384Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
resource_management ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636346}}]},
 {bucket,[{resident_ratio,[{enabled,false},
                           {couchstore_minimum,1},
                           {magma_minimum,0.2}]},
          {data_size,[{enabled,false},
                      {couchstore_maximum,2},
                      {magma_maximum,16}]}]},
 {index,[]},
 {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
 {disk_usage,[{enabled,false},{maximum,96}]},
 {collections_per_quota,[{enabled,false},{maximum,1}]}]
[ns_server:debug,2024-10-08T19:52:26.400Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/analytics/settings/config">>} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636346}}]}|
 <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]
[ns_server:debug,2024-10-08T19:52:26.409Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2024-10-08T19:52:26.423Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636346}}]}|
 <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_"...>>]
[ns_server:debug,2024-10-08T19:52:26.451Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1447]Time left before client cert regeneration: 70588778000
[ns_server:debug,2024-10-08T19:52:26.455Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636346}}]}|
 <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.cu"...>>]
[ns_server:debug,2024-10-08T19:52:26.457Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{2,63895636346}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"0A3+iOabr5BI6CxbgKqo0WJ3KvO7cnUTVxsDRyGb1Zw=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"vf/S6FrjUpLAetEeKYqriw==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"5bZLNK091CMtMaspNBx1PX8BwzMbtCuxRy/6oBdYCE2tKnxfyz1vhYdVQeYMmRXI0JoGVIrMWI4VmAoRyv7fJQ==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"Oc5Rwm916iQD2fgVb3a1GKhWeH5pAljmxlYyB18pCKs=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"is7Im5RrIP9ecAWYcYjUlEYQ8tzcupKL0gR+iQLrq2g=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"JtC1P9F2AzLN7aH4kX6D8aqVxYpo1TRi3CbgaMy5USM=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"wQz95r5jVJZqyrqXYk5+AzMZiz4=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"B27b75qqqoO60+9xrUYKN3OPES5tyCyEkEx114KQYMY=">>}}]}}]}}]
[ns_server:debug,2024-10-08T19:52:26.511Z,ns_1@cb.local:memcached_permissions<0.699.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2024-10-08T19:52:26.515Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:warn,2024-10-08T19:52:26.583Z,ns_1@cb.local:<0.1071.0>:ns_memcached:connect:1457]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2024-10-08T19:52:26.587Z,ns_1@cb.local:<0.1071.0>:memcached_refresh:do_refresh:171]Failed to connect to memcached: couldnt_connect_to_memcached
[ns_server:debug,2024-10-08T19:52:26.589Z,ns_1@cb.local:memcached_passwords<0.696.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2024-10-08T19:52:26.589Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:update_refresh_state:137]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2024-10-08T19:52:26.591Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:init:93]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2024-10-08T19:52:26.594Z,ns_1@cb.local:<0.903.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":16,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:warn,2024-10-08T19:52:26.603Z,ns_1@cb.local:<0.1073.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2024-10-08T19:52:26.611Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2024-10-08T19:52:26.613Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:init:97]activated memcached port server
[ns_server:debug,2024-10-08T19:52:26.636Z,ns_1@cb.local:memcached_passwords<0.696.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2024-10-08T19:52:26.636Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:info,2024-10-08T19:52:26.646Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[error_logger:info,2024-10-08T19:52:26.693Z,ns_1@cb.local:ns_orchestrator_child_sup<0.1075.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.1078.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2024-10-08T19:52:26.696Z,ns_1@cb.local:<0.1077.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:info,2024-10-08T19:52:26.716Z,ns_1@cb.local:ns_orchestrator_child_sup<0.1075.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.1079.0> on 'ns_1@cb.local'

[error_logger:info,2024-10-08T19:52:26.717Z,ns_1@cb.local:ns_orchestrator_child_sup<0.1075.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.1079.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:26.730Z,ns_1@cb.local:ns_orchestrator_child_sup<0.1075.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.1080.0> on 'ns_1@cb.local'

[error_logger:info,2024-10-08T19:52:26.731Z,ns_1@cb.local:ns_orchestrator_child_sup<0.1075.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.1080.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2024-10-08T19:52:26.731Z,ns_1@cb.local:ns_orchestrator_child_sup<0.1075.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.1081.0> on 'ns_1@cb.local'

[error_logger:info,2024-10-08T19:52:26.731Z,ns_1@cb.local:ns_orchestrator_child_sup<0.1075.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.1081.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:26.732Z,ns_1@cb.local:ns_orchestrator_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.1075.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:26.732Z,ns_1@cb.local:<0.1082.0>:auto_failover:init:223]init auto_failover.
[user:info,2024-10-08T19:52:26.743Z,ns_1@cb.local:<0.1082.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2024-10-08T19:52:26.743Z,ns_1@cb.local:<0.1082.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:debug,2024-10-08T19:52:26.743Z,ns_1@cb.local:<0.1082.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:debug,2024-10-08T19:52:26.759Z,ns_1@cb.local:<0.1082.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:debug,2024-10-08T19:52:26.765Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{3,63895636346}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {disable_max_count,false},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:info,2024-10-08T19:52:26.767Z,ns_1@cb.local:ns_orchestrator_sup<0.1018.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.1082.0> on 'ns_1@cb.local'

[error_logger:info,2024-10-08T19:52:26.767Z,ns_1@cb.local:ns_orchestrator_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.1082.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:26.774Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1018.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:26.787Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([auto_failover_cfg]..)
[ns_server:info,2024-10-08T19:52:26.831Z,ns_1@cb.local:mb_master_sup<0.1005.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.1095.0> on 'ns_1@cb.local'

[error_logger:info,2024-10-08T19:52:26.832Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1095.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:26.833Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1096.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:26.894Z,ns_1@cb.local:chronicle_kv_log<0.666.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"05f558c2457ead34373f1c979f713946">>,7})
[]
[ns_server:debug,2024-10-08T19:52:26.903Z,ns_1@cb.local:guardrail_enforcer<0.1097.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[error_logger:info,2024-10-08T19:52:26.903Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1097.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:26.914Z,ns_1@cb.local:<0.1099.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2024-10-08T19:52:26.915Z,ns_1@cb.local:mb_master_sup<0.1005.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.1099.0> on 'ns_1@cb.local'

[error_logger:info,2024-10-08T19:52:26.915Z,ns_1@cb.local:mb_master_sup<0.1005.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1099.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:26.916Z,ns_1@cb.local:leader_registry_sup<0.1000.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.1003.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:26.917Z,ns_1@cb.local:leader_services_sup<0.996.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.1000.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:26.917Z,ns_1@cb.local:<0.995.0>:restartable:start_child:92]Started child process <0.996.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2024-10-08T19:52:26.918Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.995.0>},
              {id,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:26.919Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1101.0>},
              {id,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:26.919Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1103.0>},
              {id,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:26.919Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1104.0>},
              {id,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:26.927Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1105.0>},
              {id,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:26.934Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1106.0>},
              {id,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.002Z,ns_1@cb.local:health_monitor_sup<0.1108.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1139.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.003Z,ns_1@cb.local:health_monitor_sup<0.1108.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1141.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:27.005Z,ns_1@cb.local:health_monitor_sup<0.1108.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1142.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.30770475>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.017Z,ns_1@cb.local:health_monitor_sup<0.1108.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1148.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.024Z,ns_1@cb.local:health_monitor_sup<0.1108.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1155.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.024Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1108.0>},
              {id,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:27.040Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1158.0>},
              {id,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.053Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1159.0>},
              {id,kv_hibernation_agent},
              {mfargs,{kv_hibernation_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.071Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1160.0>},
              {id,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2024-10-08T19:52:27.083Z,ns_1@cb.local:cb_creds_rotation<0.1162.0>:cb_creds_rotation:start_rotate_timer:214]Starting creds rotation timer (1800000ms)
[error_logger:info,2024-10-08T19:52:27.083Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1162.0>},
              {id,creds_rotation},
              {mfargs,{cb_creds_rotation,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:error,2024-10-08T19:52:27.083Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {noproc,{gen_statem,call,[mb_master,master_node,infinity]}}
    offender: [{pid,<0.930.0>},
               {id,ns_server_stats},
               {mfargs,{ns_server_stats,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.086Z,ns_1@cb.local:ns_server_sup<0.658.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1164.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.086Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.658.0>},
              {id,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:27.087Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[ns_server:debug,2024-10-08T19:52:27.087Z,ns_1@cb.local:menelaus_barrier<0.299.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.292.0>
[ns_server:debug,2024-10-08T19:52:27.089Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2024-10-08T19:52:27.089Z,ns_1@cb.local:<0.291.0>:restartable:start_child:92]Started child process <0.292.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2024-10-08T19:52:27.090Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.291.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2024-10-08T19:52:27.130Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2024-10-08T19:52:27.131Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[error_logger:info,2024-10-08T19:52:27.138Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.1168.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.169Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.1169.0>},
              {id,ns_gc_runner},
              {mfargs,{ns_gc_runner,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2024-10-08T19:52:27.169Z,ns_1@cb.local:root_sup<0.215.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.236.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2024-10-08T19:52:27.169Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@cb.local'

[ns_server:debug,2024-10-08T19:52:27.170Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]110: Entered child_loop
[ns_server:debug,2024-10-08T19:52:27.171Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]110: Entered child_loop
[ns_server:debug,2024-10-08T19:52:27.210Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.1171.0>:json_rpc_connection:init:71]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.1171.0>
[ns_server:warn,2024-10-08T19:52:27.390Z,ns_1@cb.local:<0.929.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:warn,2024-10-08T19:52:27.391Z,ns_1@cb.local:<0.885.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2024-10-08T19:52:27.517Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.1176.0>:json_rpc_connection:init:71]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.1176.0>
[ns_server:debug,2024-10-08T19:52:27.517Z,ns_1@cb.local:menelaus_cbauth<0.820.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.1176.0>} started
[ns_server:debug,2024-10-08T19:52:27.574Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2024-10-08T19:52:27.588Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/sequences_cache/revision">>} ->
[{'_vclock',[{<<"ad4331accf06ea1cb0fdd46980abac3e">>,{1,63895636347}}]}|
 <<"0">>]
[ns_server:debug,2024-10-08T19:52:27.607Z,ns_1@cb.local:ns_config_rep<0.713.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/query/sequences_cache/revision">>}]..)
[ns_server:warn,2024-10-08T19:52:27.636Z,ns_1@cb.local:<0.1073.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:warn,2024-10-08T19:52:27.644Z,ns_1@cb.local:<0.1183.0>:ns_memcached:connect:1457]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2024-10-08T19:52:27.645Z,ns_1@cb.local:<0.1183.0>:memcached_refresh:do_refresh:171]Failed to connect to memcached: couldnt_connect_to_memcached
[ns_server:debug,2024-10-08T19:52:27.645Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:update_refresh_state:137]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2024-10-08T19:52:27.745Z,ns_1@cb.local:<0.1082.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@cb.local',<<"ad4331accf06ea1cb0fdd46980abac3e">>} state new -> up
[ns_server:info,2024-10-08T19:52:27.852Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:info,2024-10-08T19:52:27.856Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:push_tls_config:233]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2024-10-08T19:52:27.873Z,ns_1@cb.local:memcached_config_mgr<0.894.0>:memcached_config_mgr:push_tls_config:237]Successfully pushed TLS config to memcached
[ns_server:debug,2024-10-08T19:52:28.171Z,ns_1@cb.local:ns_gc_runner<0.1169.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=594, prevMaxGcDuration=0 us
[ns_server:debug,2024-10-08T19:52:28.643Z,ns_1@cb.local:<0.903.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":19,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]}}">>
[ns_server:debug,2024-10-08T19:52:28.652Z,ns_1@cb.local:<0.1217.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac,isasl]
[ns_server:debug,2024-10-08T19:52:28.661Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2024-10-08T19:52:40.810Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2024-10-08T19:52:55.656Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:52:55.656Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:52:55.658Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:52:55.663Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:53:19.977Z,ns_1@cb.local:ldap_auth_cache<0.409.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2024-10-08T19:53:25.669Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:53:25.670Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:53:25.670Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:53:25.670Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:53:55.671Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:53:55.672Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:53:55.672Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:53:55.672Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:54:25.674Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:54:25.674Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:54:25.674Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:54:25.674Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:54:34.983Z,ns_1@cb.local:ldap_auth_cache<0.409.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2024-10-08T19:54:55.675Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:54:55.676Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:54:55.679Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:54:55.680Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:55:25.674Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:55:25.675Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:55:25.676Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:55:25.676Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:55:49.981Z,ns_1@cb.local:ldap_auth_cache<0.409.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2024-10-08T19:55:55.682Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:55:55.682Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:55:55.682Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:55:55.682Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:56:25.682Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:56:25.683Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:56:25.683Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:56:25.683Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:56:55.682Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:56:55.682Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:56:55.683Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2024-10-08T19:56:55.683Z,ns_1@cb.local:compaction_daemon<0.986.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2024-10-08T19:57:04.986Z,ns_1@cb.local:ldap_auth_cache<0.409.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
